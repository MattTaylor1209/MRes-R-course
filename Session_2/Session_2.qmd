---
title: "Session 2 — The tidyverse continued: import, tidying, transformation"
author: "Matthew Taylor"
execute:
  dpi: 300
  cache: false
format: 
  html:
    toc: true
    self-contained: true
editor: visual
---

# Setup code

As we did in session 1, run the following chunk (click the green arrow in the corner) which will check that everything we need for today's session is installed and up to date.

```{r setup}
#| echo: false
# Required packages
required_packages <- c("tidyverse", "arrow", "babynames", "curl", "duckdb", "gapminder", 
    "ggrepel", "ggridges", "ggthemes", "hexbin", "janitor", "Lahman", 
    "leaflet", "maps", "nycflights13", "openxlsx", "palmerpenguins", 
    "repurrrsive", "tidymodels", "writexl")

# Install BiocManager if needed
if (!requireNamespace("BiocManager", quietly = TRUE)) {
  install.packages("BiocManager")
}

# Try BiocManager first, then fallback to install.packages
for (pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    message("Trying BiocManager::install('", pkg, "')")
    tryCatch({
      BiocManager::install(pkg, ask = FALSE, update = FALSE)
    }, error = function(e_bioc) {
      message("BiocManager failed. Trying install.packages('", pkg, "')")
      tryCatch({
        install.packages(pkg, dependencies = TRUE)
      }, error = function(e_cran) {
        message("Failed to install '", pkg, "' via both methods.")
      })
    })
  }
}


```

Also, before we start, click on the little gear above the document window and select "clear all output" from the dropdown menu (this prevents any *spoilers!*):

![](images/clipboard-2475862271.png)

# Welcome

Welcome to part 2 of the *foundations of R for bioinformatics* course. Today, we are going to hone our tidyverse skills as we learn how to import, tidy and transform data. You may already wondering what is meant by "tidying" data - can data be *un*tidy? The answer, from the perspective of R, is yes! But more on that later...

We will also take a closer look at an important tidyverse tool that you have already seen in action during session 1 but that I haven’t yet explained properly: the **pipe**. The pipe, written as `%>%` or `|>`, allows us to link together a sequence of functions so that the output of one step becomes the input to the next. Instead of writing long, nested commands that are difficult to read, we can use the pipe to build analyses step by step in a way that mirrors how we think about the task: *take this dataset, then filter it, then summarise it, then plot it.* Using the pipe makes your code more readable, more reproducible, and easier to debug.

As before, this session is heavily inspired by [*R for Data Science(2nd Edition)*](https://r4ds.hadley.nz/) by Hadley Wickham & Garrett Grolemund. However, **I WILL ALSO BE INCLUDING SOME REAL WORLD RNA SEQ DATA FROM REF** to make the course more applicable to our needs — namely to introduce you to how R can be used for big data/bioinformatics, such an an RNAseq experiment.

## Goals for session 2

By the end of this session, you will be able to:

-   Import data into R from different file types.

-   Explain what “tidy data” means and why tidying is important for analysis.

-   Understand the difference between wide and long data.

-   Apply common tidyverse functions such as `mutate()`, `select()`, `filter()`, `arrange()`, and `summarise()` to explore and transform data.

-   Identify and correct issues such as missing values and incorrect variable types.

-   Understand the pipe (`%>%`) and how to use it to build clear, readable, reproducible code.

## Packages required

We are going to be using `dplyr`, `readr` and `tidyr` which are all part of the `tidyverse` package, so loading that will be sufficient. We're going to use some of our own data for this session (which we will load later), and `ggplot2` for plotting (also part of `tidyverse`). Note: these were all installed in our setup chunk, so we just need to load them using the `library` function now:

```{r}
library(tidyverse)
```

We've been ignoring the output of this till now, but let's just quickly look at what this is telling us.

![](images/clipboard-572882515.png)

This is a warning that the package in question was compiled in a different version of R to the one we are running. Don’t panic — warnings are not errors. If your code runs and plots appear, you’re fine. (The way you would address this would be to update your R installation and re-install the packages; there is no need for us to do this now).

![](images/clipboard-77745967.png)

This part is telling us that the `dplyr` package overwrites some functions in base R. If you want to use the base version of these functions after loading dplyr, you’ll need to use their full names: `stats::filter()` and `stats::lag()`. So far, we’ve mostly ignored which package a function comes from because it doesn’t usually matter. However, knowing the package can help you find help and find related functions, so when we need to be precise about which package a function comes from, we’ll use the same syntax as R: `packagename::functionname()`.

# Data import

## Introduction

This section is adapted from [*Chapter 7*](https://r4ds.hadley.nz/data-import.html) of the [*R for Data Science(2nd Edition)*](https://r4ds.hadley.nz/) course by Hadley Wickham & Garrett Grolemund. For the sake of time, we will only cover the sections which are most relevant for our needs today, but I would encourage you to take a look at that Chapter for further details—particularly the section on controlling column types as we will not have time for that today.

Up to now, we have been using datasets that come pre-built into R packages. This is a great way for learning basic visualisations etc, but not reflective of your real-world use of R. In the following section, we are going to be working with some real-world data from the UK's Civil Aviation Authority, published online. Therefore, we need to equip ourselves with the tools necessary for reading this data into R. For this, we are going to use the **readr** package, which is part of the tidyverse.

## Reading data from a file

In R, you’ll most often work with data that comes in the form of tables—rows and columns of values, much like a spreadsheet. These tables are usually stored in plain text files, where special characters (called *delimiters*, such as commas, tabs, or spaces) separate the values in each row. The most common examples you’ll encounter are CSV (comma-separated values) files, TSV (tab-separated values) files, and sometimes Excel spreadsheets.

We will take the most common format as an example: the CSV. To read a file like this into R, we use the function`read_csv()` from `readr`. The first argument this function needs is a file name, which can be the full name of a file in your working directory (or sub-directory) in quotation marks, e.g. `read_csv("some_file.csv")`. Note that we have to include the ".csv" part. Alternatively, if the data is hosted online somewhere, you can provide the URL (which is true in our example case here):

```{r}
students <- read_csv("https://pos.it/r4ds-students-csv")
```

When you use `read_csv()`, R prints a message that summarises the imported data. It tells you how many rows and columns were read, which delimiter was used, and the data types assigned to each column. It also explains how to see the full column specification and how to suppress the message if you prefer. This output is a built-in feature of the `readr` package.

After reading data into R, you will normally have to transform it in some way to make it easier to work with. We will come onto more advanced transformation tips later. For now, let's look at what we have just imported:

```{r}
students
```

In the `favourite.food` column, there are a bunch of food items, and then the character string `N/A`, which should have been a real `NA` that R will recognize as “not available”. This is something we can address using the `na` argument. By default, `read_csv()` only recognizes empty strings (`""`) in this dataset as `NA`s, and we want it to also recognize the character string `"N/A"`. We can do this by specifying the `na` argument in `read_csv()`:

```{r}
students <- read_csv("https://pos.it/r4ds-students-csv", na = c("N/A", ""))
```

There's another issue with this dataset which is unfortunately hidden by Quarto - the columns "Student ID" and "Full Name" contain spaces, breaking R’s usual rules for variable names; they’re **non-syntactic** names. If you type `students` into the console, you will see how R deals with them.

![](images/clipboard-3325204219.png){width="293"}

Notice the backticks? Every time you want to refer to these columns, you would have to quote them using backticks, which is a pain. We should rename them, and we could do it manually. But way easier is to just use a function in the `janitor` package. Remember: `%>%` is our way of "piping" whatever we write on the left into a function on the right, which we will learn more about in the nextsection.

```{r}
library(janitor)

students <- students %>% clean_names()

students
```

Other issues with this data:

-   `meal_plan` is currently a "character" variable type ![](images/clipboard-4225455390.png){width="76" height="27"}when it should be a factor.

-   Some silly person typed "five" in as the age, which means now `age` has become a character variable type too, when it should be numeric.

We can fix both these problems in one go using a function called `mutate()` which we will learn more about in the *Data transformation* section:

```{r}
students <- students %>%
  janitor::clean_names() %>%
  mutate(
    meal_plan = factor(meal_plan),
    age = parse_number(if_else(age == "five", "5", age))
  )
```

A new function here is `if_else()`, which has three arguments. The first argument `test` should be a logical vector. The result will contain the value of the second argument, `yes`, when `test` is `TRUE`, and the value of the third argument, `no`, when it is `FALSE`. Here we’re saying if `age` is the character string `"five"`, make it `"5"`, and if not leave it as `age`.

### Other `read_csv` arguments

`read_csv()` has other arguments which are useful if you only want to read a certain part of the data, or skip rows which do not contain useful information. You can find out more using `?read_csv()` or at [*Chapter 7*](https://r4ds.hadley.nz/data-import.html) of the Hadley course. But to quickly demonstrate a couple here, if your data doesn't have column names in the first row, then you don't want `read_csv()` to treat the first row as column names, which is its default behaviour. To stop this, use `col_names = FALSE`:

```{r}
read_csv(
  "1,2,3
  4,5,6", ## these 2 rows are just making a dummy csv file with 1,2,3, as the first row and 4,5,6 as the second
  col_names = FALSE # will just name the columns X1, X2....Xn
)
```

Alternatively, you can pass `col_names` a character vector which will be used as the column names:

```{r}
read_csv(
  "1,2,3
  4,5,6",
  col_names = c("x", "y", "z")
)
```

## Reading data from multiple files

Sometimes the data you are working from might be split across multiple files. Later on, we are going to be working on data from the CAA on UK airport statistics. The CAA publishes this data monthly. We are only going to be looking at one month's data (February 2025) but if we wanted to combine multiple months, `read_csv()` lets us do that:

```{r}
jan_feb_flights <- c(
  "https://www.caa.co.uk/Documents/Download/23995/23907051-3e36-4e54-b7aa-53d0296849df/16963",
  "https://www.caa.co.uk/Documents/Download/23996/6890c139-9322-4a92-9e12-b6c70d505aa7/17059"
)

read_csv(jan_feb_flights, id = "file")
```

The `id` argument adds a new column called `file` to the resulting data frame that identifies the file the data come from. This is especially helpful in circumstances where the files you’re reading in do not have an identifying column that can help you trace the observations back to their original sources.

## Writing to a file

If you want to save your data back to disk, you can use the function `write_csv()`. The most important arguments to this function are `x` (the data frame to save) and `file` (the location to save it). You can also specify how missing values are written with `na`, and if you want to `append` to an existing file.

```{r}
write_csv(students, "students-2.csv")
```

Now let’s read that csv file back in. Note that the variable type information that you just set up is lost when you save to CSV because you’re starting over with reading from a plain text file again:

```{r}
students
```

```{r}
read_csv("students-2.csv")
```

This makes CSVs a little unreliable for caching interim results—you need to recreate the column specification every time you load in. As an alternative:

`write_rds()` and `read_rds()` store data in R’s custom binary format called RDS. This means that when you reload the object, you are loading the *exact same* R object that you stored. We will be working with RDS files in the next session as they are useful ways of storing count data from RNAseq experiments which have been performed using R-based tools.

```{r}
write_rds(students, "students.rds")
read_rds("students.rds")
```

# Data transformation

## Introduction

In the previous session, we used the `penguins` dataset to create some useful visualisations of different relationships within the data. Often, however, the data you have is not exactly in the correct format to give the insight you want. Maybe the variables are named incorrectly (which we encountered briefly in the previous section), or you need to summarise the data in a particular way. Here, we will learn how to **transform** data using the **dplyr** package.

### Our dataset

In the Hadley course section on data transformation, they use some built-in data: this time, a dataset on flights which departed New York City in 2013. I've decide to adapt this to instead use data from the UK's own [Civil Aviation Authority (CAA)](https://www.caa.co.uk/data-and-analysis/) - specifically statistics on UK airports from February 2025.

To get the dataset, we can import it directly from their website using the function `read_csv()` from the `tidyverse` package. Remember: using `<-` assigns the code on the right to an object name on the left. There's nothing special about calling it `uk_flights` — we could call it `bananas` if we really wanted — but `uk_flights` is a more useful name for what the data actually is.

```{r import_uk_flights}
uk_flights <- read_csv("https://www.caa.co.uk/Documents/Download/23996/6890c139-9322-4a92-9e12-b6c70d505aa7/17059")

# Fallback in case the URL doesn't work - you can download the data into your current working directory and read it in like this:
# uk_flights <- read_csv("Table_03_Aircraft_Movements.csv")

# I'm also going to remove the first 2 columns, which aren't useful for us:
uk_flights <- uk_flights[, -(1:2)] # this says "subset uk_flights without columns 1 to 2
```

We're also going to convert it quickly into a **tibble.** A tibble is a special type of data frame used by the tidyverse to avoid some common problems. The most important difference between tibbles and data frames is the way tibbles print; they are designed for large datasets, so they only show the first few rows and only the columns that fit on one screen. There are a few options to see everything: `View(uk_flights)` opens an interactive, scrollable, and filterable view. Otherwise you can use `print(uk_flights, width = Inf)` to show all columns, or use `glimpse()`:

```{r convert_and_view}

uk_flights <- tibble(uk_flights)
glimpse(uk_flights)
```

The variable names are followed by abbreviations that tell you the **type** of each variable, which we talked a bit about in session 1: `<int>` is short for integer, `<dbl>` is short for double, `<chr>` for character, and `<dttm>` for date-time. The operations you can perform on a column depend heavily on its “type.”

### dplyr basics

The following sections will introduce you to the fundamental dplyr functions which will solve the majority of your data manipulation challenges. All of these functions have the following in common:

-   The first *argument* of the function (input) is always a data frame

-   The subsequent arguments typically describe which column you want the function to operate on, using the variable name without quotes, e.g., `reporting_airport_name`

-   The output is always a new data frame.

Each function typically does just one thing. Therefore, complex problems typically involve combining these functions one after another, which we do using the **pipe**. I know I keep mentioning this thing without fully explaining it — I'm sorry! — but I promise we will cover this in more detail soon. In brief, the pipe takes the thing on the left and passes it into the function on the right. Think of it as the word "then". That makes it possible to get a sense of the following code even though you haven’t yet learned the details: **Pause: what do you think the output of the following chunk will be? Read it as if reading a sentence "take the uk_flights data frame, then..."**

```{r}
uk_flights %>%
  group_by(reporting_airport_group_name) %>%
  summarise(
    mean_total_flights = round(mean(grand_total, na.rm = TRUE))
  )

```

dplyr’s verbs are organized into four groups based on what they operate on: **rows**, **columns**, **groups**, or **tables**. We will cover these one by one!

## Rows

The 2 most important functions here are:

-   `filter()` - changes which rows are present without changing their order

-   `arrange()` - changes the order of the rows without changing which are present

The columns are left unchanged. Another function is `distinct()` which finds rows with unique values (and can optionally modify the columns, unlike the other 2).

### `filter()`

`filter()` allows you to keep rows based on the values of the columns. The first argument is the data frame. The second and subsequent arguments are the conditions that must be true to keep the row. For example, we could find all airports which reported more than 1000 flights in our dataset.

```{r}
uk_flights %>%
  filter(grand_total > 1000)
```

Notice how we now have a preview of a new data table that satisfies the requirements of our filter — all of the values in the grand_total column are greater than 1000. Note that this hasn't changed our original data (we haven't overwritten it) and we also haven't saved it to a new object, so it literally only exists as a preview below that chunk. If we wanted to overwrite our original (not recommended usually), we would put `uk_flights <-` before that code; or, more likely, to save to a new object, we would put in a new variable name there e.g., `busy_airports <-`:

```{r}
busy_airports <- uk_flights %>%
  filter(grand_total > 1000)
```

This is the key difference between the *assignment operator `<-`* and the pipe `%>%`.

-   `<-` means assign what's on the right to the thing on the left.

-   `%>%` means take what's on the left and do the thing on the right to it.

Back to `filter()`...

As well as `>` (greater than), you can use `>=` (greater than or equal to), `<` (less than), `<=` (less than or equal to), `==` (equal to), and `!=` (not equal to). You can also combine conditions with `&` or `,` to indicate “and” (check for both conditions) or with `|` to indicate “or” (check for either condition):

```{r}
# Airports with more than 1000 flights but less than 2000:
uk_flights %>%
  filter(grand_total > 1000 & grand_total < 2000)
```

```{r}
# Airports which are in London or have more flights than 1000
uk_flights %>%
  filter(
    grand_total > 1000 | reporting_airport_group_name == "London Area Airports")
```

```{r}
# All airports not in London area
uk_flights %>%
  filter(
    reporting_airport_group_name != "London Area Airports")
```

There’s a useful shortcut when you’re combining `|` and `==`, which is `%in%`. It keeps rows where the variable equals one of the values on the right:

```{r}
# Data for Birmingham and Stansted airports
uk_flights %>%
  filter(
    reporting_airport_name %in% c("BIRMINGHAM", "STANSTED")
  )
```

Note that when you want `filter()` to check if something is equal to something, you have to use the double equals `==` rather than single `=`. Basically, this is because R sometimes treats `=` like the assignment operator (as in other languages).

Another mistakes is writing “or” statements like you would in English. We might read the following as "filter the airport names to only include them if they are Birmingham or Stansted." With some variables, this might not throw an error (but still won't give you what you meant). In our case it does:

```{r}
#| error: true
uk_flights %>%
  filter(
    reporting_airport_name == "BIRMINGHAM" | "STANSTED"
  )
```

### `arrange()`

`arrange()` changes the order of the rows based on the value of the columns. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of the preceding columns. In addition, you can use the `desc()` function on a column inside of `arrange()` to re-order the data frame based on that column from largest number to smallest. Say we wanted to sort our airports by the number of military flights:

```{r}
uk_flights %>%
  arrange(desc(military))
```

How would we go from smallest to largest? It turns out, that is the default for `arrange()`. So if we remove `desc()` from the previous example:

```{r}
uk_flights %>%
  arrange(military)
```

Note that the number of rows has not changed – we’re only arranging the data, we’re not filtering it.

**What if we wanted to arrange by number of military flights from lowest to highest, and in the event of ties, to arrange by total number of flights from highest to lowest?**

.

.

.

*Scroll down for answer*

.

.

.

.

.

.

.

.

.

.

```{r}
uk_flights %>%
  arrange(military, desc(grand_total))
```

### **`distinct()`**

`distinct()` finds all the unique rows in a dataset, however our dataset is not the best for demonstrating this function as the rows are mostly all unique. If there are any duplicate rows in the data, you can use `distinct()` with no arguments to remove them:

```{r}
# Remove duplicate rows, if any
uk_flights %>%
  distinct()
```

We have no duplicates, so the number of rows isn't changed.

What if we just want to know what all of the different reporting airport groups are?

```{r}
uk_flights %>%
  distinct(reporting_airport_group_name)
```

Notice how we've lost the other columns? This is fine in our case because we just wanted to know the unique values of the reporting_airport_group_name variable. But it is possible to keep the other columns in place using `.keep_all = TRUE`

```{r}
uk_flights %>%
  distinct(reporting_airport_group_name, .keep_all = TRUE)
```

Most of the time, however, you’ll want the distinct combination of some variables, so you can also optionally supply column names. Say now we want to find all the unique combinations of reporting_area_group_name and reporting_airport_name:

```{r}
uk_flights %>%
  distinct(reporting_airport_group_name, reporting_airport_name)
```

I.e,. we now have a quick overview of all of the airports in our dataset and their reporting area group name.

What if we want to know how many of each group there are? In this case, we're betting off using `count()` and we can sort from highest to lowest using `sort = TRUE`.

```{r}
uk_flights %>%
  count(reporting_airport_group_name, sort = TRUE)
```

## Columns

There are 4 important functions which affect the columns without changing the rows:

-   `select()` changes which columns are present

-   `mutate()` creates new columns derived from existing columns

-   `rename()` changes the names of columns

-   `relocate()` changes the positions of columns

### `select()`

`select()` is sort of like the columns version of `filter()` — rather than filtering for rows which fit a certain criteria, we are selecting specific columns.

Select columns by name:

```{r}
uk_flights %>%
  select(reporting_airport_name, grand_total)
```

Select all columns within a range (inclusive):

```{r}
uk_flights %>%
  select(reporting_airport_group_name:air_transport)
```

Select all columns *except* for those within a range:

```{r}
uk_flights %>%
  select(!reporting_airport_group_name:air_transport)
```

`!` reads like "NOT".

Select columns of a specific variable type, e.g., characters

```{r}
uk_flights %>%
  select(where(is.character))
```

There are a number of helper functions you can use within `select()`:

-   `starts_with("abc")`: matches names that begin with “abc”.

-   `ends_with("xyz")`: matches names that end with “xyz”.

-   `contains("ijk")`: matches names that contain “ijk”.

-   `num_range("x", 1:3)`: matches `x1`, `x2` and `x3`.

You can rename variables as you `select()` them by using `=`. The new name appears on the left-hand side of the `=`, and the old variable appears on the right-hand side.

```{r}
uk_flights %>%
  select(airport_name = reporting_airport_name)
```

### `mutate()`

This is a really useful function particularly for calculating new metrics from your data. E.g., suppose you wanted to calculate the percentage of flights at an airport which are related to business aviation. You give the column a name first (`business_percentage`) and write a formula relating to other columns to calculate it:

```{r}
uk_flights %>% 
  mutate(
    business_percentage = business_aviation / grand_total * 100
  )
```

Now we can see that column at the end of the preview. (Note we still haven't changed `uk_flights` itself as we didn't overwrite it using the assignment operator `<-`).

By default, `mutate()` adds new columns on the right-hand side of your dataset, which makes it difficult to see what’s happening here. We can use the `.before` argument to instead add the variables to the left-hand side:

```{r}
uk_flights %>% 
  mutate(
    business_percentage = business_aviation / grand_total * 100,
    .before = 3 # means put it before the third column
  )
```

As you may have guessed, to do the opposite, we use `.after`. You also can specify a particular column rather than just a column number, e.g., if we want to put the new `business_percentage` column after `grand_total`:

```{r}
uk_flights %>% 
  mutate(
    business_percentage = business_aviation / grand_total * 100,
    .after = grand_total 
  )
```

Another useful argument for `mutate()` is `.keep` which allows you to keep certain variables in the final data frame — a great use for this is to specify `"used"` which will only keep the variables used to make the new column:

```{r}
uk_flights %>% 
  mutate(
    business_percentage = business_aviation / grand_total * 100,
    .keep = "used" 
  )
```

However, now we can't see what airport we're referring to! **How could we fix this? (hint: will need to use 2 functions (`mutate()` and `select()`) and pipes).**

.

.

.

*Scroll down for answer*

.

.

.

*.*

.

.

.

.

.

.

`.keep` *only accepts one of these fixed keywords:*

-   `"all"` *(keep all existing columns, the default)*

-   `"used"` *(keep only columns used to make the new variables)*

-   `"unused"` *(drop those used, keep the rest)*

-   `"none"` *(drop all original columns, keep only the new ones)*

*So you would have to do this sequentially using `mutate()` then `select()`:*

```{r}
uk_flights %>% 
  mutate(
    business_percentage = business_aviation / grand_total * 100) %>% 
  select(
    reporting_airport_name, business_aviation, grand_total, business_percentage)
```

### `rename()`

`select()` removes all of the other columns you haven't selected. If you want to keep them all but just rename a few:

```{r}
uk_flights %>%
  rename(
    airport_name = reporting_airport_name, 
    reporting_group = reporting_airport_group_name)
```

If you have a bunch of inconsistently named columns and it would be painful to fix them all by hand, check out `janitor::clean_names()` which provides some useful automated cleaning.

### `relocate()`

`relocate()` moves columns around. You might want to collect related variables together or move important variables to the front. By default `relocate()` moves variables to the front:

```{r}
uk_flights %>%
  relocate(grand_total, business_aviation)
```

You can also specify where to put them using the `.before` and `.after` arguments, just like in `mutate()`:

```{r}
uk_flights %>%
  relocate(official:business_aviation, .after = reporting_airport_name)
```

## The pipe

The **pipe** is one of the most powerful tools in the tidyverse. It takes the result of one step and passes it as the first argument to the next function. You’ve already seen simple examples, but its *real power* appears when we want to combine multiple steps.

Suppose we want to find the airport, outside of the London area, with the highest percentage of official flights. To do this, we need to:

-   `filter()` for airports not in the *London Area Airports* category

-   Create a new column (`mutate()`) giving us the percentage of official flights

-   `select()` just the columns we want to show, and

-   `arrange()` by the percentage of official flights from high to low.

Without the pipe, we have two options:

1.  **Nesting functions** (ugly, hard to read, and a pain to type):

```{r}
arrange(
  select(
    mutate(
      filter(
        uk_flights, # here is our data frame - look how tucked away it is!!
        reporting_airport_group_name != "London Area Airports"
      ),
      official_percentage = official / grand_total * 100
    ),
    reporting_airport_name, official_percentage
  ),
  desc(official_percentage)
)
```

2.  **Creating lots of intermediate objects** (clearer, but verbose):

```{r}
uk_flights_1 <- filter(uk_flights, reporting_airport_group_name != "London Area Airports")
uk_flights_2 <- mutate(uk_flights_1, official_percentage = official / grand_total * 100)
uk_flights_3 <- select(uk_flights_2, reporting_airport_name, official_percentage)
arrange(uk_flights_3, desc(official_percentage))
```

If we use the pipe instead, we get this:

```{r}
uk_flights %>% # take our data frame uk_flights
  filter(reporting_airport_group_name != "London Area Airports") %>% # filter it
  mutate(official_percentage = official / grand_total * 100) %>% # add new column
  select(reporting_airport_name, official_percentage) %>% # select columns we want
  arrange(desc(official_percentage)) # sort by official percentage descending
  
```

This version reads like a sentence: *“Take the flights data, then filter, then mutate, then select, then arrange.”* It’s easier to type, avoids awkward brackets and commas, and because the data is piped in from the start, RStudio’s autocomplete works smoothly with column names.

## Groups

So far you’ve learned about functions that work with rows and columns. dplyr gets even more powerful when you add in the ability to work with groups. In this section, we’ll focus on the most important functions: `group_by()`, `summarize()`, and the slice family of functions.

### `.group_by()`

This function allows you to subdivide your dataset into groups which are useful for further analysis. E.g., we know we have multiple airports per *reporting_airport_group_name* so we could group by this variable:

```{r}
uk_flights %>%
  group_by(reporting_airport_group_name)
```

Nothing major has happened there, except we now see that all of the London Area Airports are together, all of the Non UK Reporting Airports are together etc. You may also notice the output indicates what we are grouped by: ![](images/clipboard-727697052.png){width="289"} This means that subsequent operations (if we piped further) are applied per Group. We will see this in action in the next section.

### `summarise()`

The most important grouped operation is a summary, which, if being used to calculate a single summary statistic, reduces the data frame to have a single row for each group. In dplyr, this operation is performed by `summarise()`.

```{r}
uk_flights %>%
  group_by(reporting_airport_group_name) %>%
  summarise(
    avg_total_flights = round(mean(grand_total, na.rm = TRUE))
  )
```

*Note that I have included `na.rm = TRUE`* *within the `mean()`* *function - this was just in case we had missing data, which would have resulted in NA being the output of `mean()`*. *In our dataset, we didn't have to worry about this, you may need to worry about NAs in data you get elsewhere. We don't have time to go into detail about handling missing values in this course, so for more information, see [Chapter 18](https://r4ds.hadley.nz/missing-values.html) of the Hadley course.*

You can create any number of summaries in a single call to `summarise()`; one very useful summary is `n()`, which returns the number of rows in each group:

```{r}
uk_flights %>%
  group_by(reporting_airport_group_name) %>%
  summarise(
    avg_total_flights = round(mean(grand_total, na.rm = TRUE)),
    n = n()
  )
```

And with any good summary, it's always nice to visualise the data afterwards. Let's use some of the skills we learned in lesson 1 to more clearly visualise this data. **Note how we can just continue the pipe into the ggplot function - this is similar to how in session 1 we were piping `penguins`** **into ggplot by `penguins %>% ggplot()`**

```{r}
uk_flights %>%
  group_by(reporting_airport_group_name) %>%
  summarise(
    avg_total_flights = round(mean(grand_total, na.rm = TRUE)),
    n = n()
  ) %>%
  ggplot(aes(
    x = reorder(reporting_airport_group_name, avg_total_flights), # reordering lowest to highest
    y = avg_total_flights
  )) +
  geom_bar(stat = "identity") + # telling geom_bar to use the x value as identity
  labs(x = "Airport group", y = "Average total flights") + 
  theme_minimal()
```

*Think about some of the other summaries we have done and how you could represent those visually. (If we have time, we will have a go.)*

.

.

---

### The `slice_` functions

There are five handy functions that allow you to extract specific rows within each group:

-   `df |> slice_head(n = 1)` takes the first row from each group.

-   `df |> slice_tail(n = 1)` takes the last row in each group.

-   `df |> slice_min(x, n = 1)` takes the row with the smallest value of column `x`.

-   `df |> slice_max(x, n = 1)` takes the row with the largest value of column `x`.

-   `df |> slice_sample(n = 1)` takes one random row.

You can vary `n` to select more than one row, or instead of `n =`, you can use `prop = 0.1` to select (e.g.) 10% of the rows in each group.

Example: finding the busiest airports for each reporting group.

```{r}
uk_flights %>%
  group_by(reporting_airport_group_name) %>%
  slice_max(grand_total, n = 1) %>%
  relocate(grand_total)
```

This is similar to computing the busiest airport with `summarise()`, but you get the whole corresponding row (or rows if there’s a tie) instead of the single summary statistic.

### Grouping by multiple variables

You can create groups using more than one variable. Our data doesn't work particularly well in this example, so let's make a new variable first. We'll call an airport "busy" if it saw more than 5000 flights in this reporting period:

```{r}
uk_flights <- uk_flights %>% # we want to actually add this column to our data
  mutate(busy = ifelse(grand_total > 5000, "busy", "not_busy"))
```

Now let's say we want to group by the reporting airport group name and our new busy variable:

```{r}
busy_areas <- uk_flights %>%
  group_by(reporting_airport_group_name, busy) %>%
  relocate(busy, .after = reporting_airport_name)

busy_areas
```

When you summarize a tibble grouped by more than one variable, each summary peels off the last group. To make it obvious what’s happening, dplyr displays a message that tells you how you can change this behavior:

```{r}
busy_n <- busy_areas %>%
  summarise(n = n())

busy_n
```

If you’re happy with this behavior, you can explicitly request it in order to suppress the message:

```{r}
busy_n <- busy_areas %>%
  summarise(n = n(),
            avg_flights = round(mean(grand_total, na.rm = TRUE)),
            .groups = "drop_last")

busy_n
```

Alternatively, change the default behavior by setting a different value, e.g., `"drop"` to drop all grouping or `"keep"` to preserve the same groups.

### Ungrouping

You might also want to remove grouping from a data frame without using `summarize()`. You can do this with `ungroup()`.

```{r}
busy_areas %>% 
  ungroup()
```

Now let’s see what happens when you summarise an ungrouped data frame:

```{r}
busy_areas %>%
  ungroup() %>%
  summarise(
    avg_flights = round(mean(grand_total, na.rm = TRUE)),
    n = n()
  )

```

Now we just have a single row because dplyr is treating them all as belonging to one group.

### `.by`

If you want to combine some of the above steps (grouping and then summarising) into one, dplyr also includes an experimental feature called `.by`. So say you want to summarise the mean number of flights, grouping both by the area of the airport and the "busy-ness":

```{r}
uk_flights %>%
  summarise(
    avg_flights = round(mean(grand_total, na.rm = TRUE)),
    n = n(), 
    .by = c(reporting_airport_group_name, busy)
  )
```

`.by` works with all of these functions and has the advantage that you don’t need to use the `.groups` argument to suppress the grouping message or `ungroup()` when you’re done.

## Tables - using joins

Due to time constraints, I am only going to be able to briefly go over some of the dplyr functions which work on tables. For a more detailed overview, see [*Chapter 19*](https://r4ds.hadley.nz/joins.html) of the Hadley course. Their worked example, using the `nycflights13` package, is also much more involved than our one!

We shall close off this *Data transformation* section by bringing together what we have learned so far, add in the ability to use **joins**, and ask the question: **which airports in our dataset had the highest percentage of cancelled flights, per reporting region?**

------------------------------------------------------------------------

It’s rare that a data analysis involves only a single data frame. Typically you have many data frames, and you must **join** them together to answer the questions that you’re interested in.

In our case, we are using data from the CAA for the number of flights of different categories across UK airports. It would be nice also if we could see, for example, the number of cancelled flights per airport. Unfortunately, our data frame does not include this information. Fortunately, the CAA *does* record this information — but it is in a *separate* file.

Let's import it into Rstudio now:

```{r cancelled}
cancelled <- read_csv("https://www.caa.co.uk/Documents/Download/23996/6890c139-9322-4a92-9e12-b6c70d505aa7/17062")

# Fallback in case the URL doesn't work - you can download the data into your current working directory and read it in like this:
# cancelled <- read_csv("Table_04a_Cancelled_Movements.csv")

# Again, I'm also going to remove the first 2 columns, which aren't useful for us:
cancelled <- cancelled[, -(1:2)] 

# And convert to a tibble

cancelled <- tibble(cancelled)
```

We can have a look at what is in this new dataset:

```{r}
glimpse(cancelled)
```

Immediately, something might stand out to you: 2 of those column names (reporting_airport_group_name and reporting_airport_name) are identical to 2 of the columns we had in our other dataset. This is good news for joining purposes!

The most common form of join is `left_join()` which is special because the output will always have the same rows as the data frame you are joining *to*.

By default, `left_join()` will use all variables that appear in both data frames as the join key, the so called **natural** join. In our case, both `reporting_airport_group_name` and `reporting_airport_name` appear in each dataset, so the join will use both of these to try and match the datasets:

```{r}
uk_flights %>%
  left_join(cancelled)
```

Now we can see all of the other columns within `cancelled` have been added to the `uk_flights` data frame using the `reporting_airport_group_name` and `reporting_airport_name` to find matching **keys.** Note also the presence of some NAs in our data now. **Where have they come from?**

.

.

.

*Scroll down for answer*

.

.

.

.

.

.

.

.

.

*You may have noticed when looking at our new `cancelled`* *data frame that we only have 50 rows compared to the 57 before.* *To make this explicit:*

```{r}
dim(uk_flights) # 57 rows, 15 columns
dim(cancelled) # 50 rows, 5 columns
```

*This means that some of our airports didn't collect information on the number of cancelled flights — so when we join the datasets, this is reflected by the values of NA.*

We might only care about matching by one specific variable in our data. For this, we can use the `join_by()` function:

```{r}
uk_flights %>%
  left_join(cancelled,
            join_by(reporting_airport_name))
```

Additionally, we might only care to join certain columns into our data frame. We can use a pipe nested within our `left_join()` function to achieve this (note that at least one of the columns we select has to match a column in our destination dataset). We will do this to select the `total_cancelled_atms` column from the `cancelled` data frame and join it to our `uk_flights` data frame by matching `reporting_airport_name`, and make a new data frame called `uk_flights_detailed`

```{r}
uk_flights_detailed <- uk_flights %>%
  left_join(cancelled %>% select(reporting_airport_name,total_cancelled_atms))

uk_flights_detailed
```

One other thing you may have spotted is that even some of the airports that *are* included in our `cancelled` data frame have just a dash "-" instead of a number for `total_cancelled_atms`. This causes 2 problems, one of which is already present:

-   The presence of a single character element in a column causes the whole variable to become the type "character", meaning basically that numbers are not being treated as numbers.

-   The above, and the presence of "-", would therefore mess things up if we tried to do sums on this column, such as calculating the mean.

To address this, we can use `mutate()` again, but this time rather than creating a whole new column, we can just edit the `total_cancelled_atms` column. We shall replace the "-" with NA (easier to omit when doing maths) and change the whole column type to numeric. We want to overwrite our old data frame, so we will also need to use the assignment operator `<-`:

```{r}
uk_flights_detailed <- uk_flights_detailed %>%
  mutate(
    total_cancelled_atms = na_if(total_cancelled_atms, "-"),
    total_cancelled_atms = as.numeric(total_cancelled_atms)
  )
  
uk_flights_detailed
```

We are now ready to put together what we have learned to answer the question at the start of this section: **which airports in our dataset had the highest percentage of cancelled flights, per reporting region?** Have a go yourself first, and if you get stuck or want to check you are right, scroll down for a model answer.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

```{r}
uk_flights_detailed %>%
  mutate(percentage_cancelled = total_cancelled_atms / grand_total * 100) %>% # make a new column with percentage cancelled flights
  group_by(reporting_airport_group_name) %>% # group by the area
  slice_max(percentage_cancelled, n = 1) %>% # take the highest number
  select(reporting_airport_group_name, reporting_airport_name, percentage_cancelled) # select just the columns you want to display
```

*For reasons why Barra might have the highest percentage of cancelled flights, see this!* <https://www.visitouterhebrides.co.uk/see-and-do/traigh-mhor-beach-barra-airport-p561201>

# Data tidying

In the real world, data arrives in all sorts of different formats. Biologists might get spreadsheets with samples as columns and measurements as rows, statisticians might get multiple values crammed into a single cell, and social scientists might get separate sheets for different groups. These formats often make sense to humans, but they are awkward for computers to work with.

In this section, we will cover a consistent way of organising data in R using a system called **tidy data**. We’ll focus on **tidyr**, a package that provides a bunch of tools to help tidy up your messy datasets. tidyr is also part of the larger `tidyverse` package.

Tidy data is not just about neatness. Once a dataset is tidy:

-   The useful `tidyverse` functions (`filter()`, `mutate()`, `summarise()`, `ggplot()`) work consistently.

-   Code becomes shorter, clearer, and easier to share.

-   Complex workflows (RNA-seq counts, clinical metadata, survey responses) become easier to visualise and analyse.

-   Variables are placed into columns, which allows R's vectorised nature (it's main advantage over other programming languages) to shine.

## Tidy data

The 3 tables below show the exact same information. Each shows the same values of four variables: *country*, *year*, *population*, and number of documented *cases* of TB (tuberculosis). The only difference between them is *how* they organise the data, and each are likely equally intelligible to us, but vastly different in the eyes of R!

```{r}
table1
```

```{r}
table2
```

```{r}
table3
```

Only the first, `table1`, will be easy to work with using the `tidyverse`, because it is in fact **tidy.** What do we mean by this?

There are 3 interrelated rules which make a dataset tidy:

1.  Each **variable** is a **column**; each **column** is a **variable**.

2.  Each **observation** is a **row**; each **row** is an **observation**.

3.  Each **value** is a **cell**; each **cell** is a single **value**.

With this in mind, **can you see how `table2` and `table3` violate the tidiness rules?**

.

.

.

*Scroll down for answer*

.

.

.

.

.

.

.

.

.

.

-   *`table2` only satisfies rule 3. Each column isn't a different variable, as "type" and "count" relate to 2 separate things (total population and total TB cases): violating rule 1. Each row isn't a different observation as it looks like there are 2 separate observations per year, when in reality there is only 1: violating rule 2.*

-   *`table3`* *satisfies rule 2 as each row is seemingly a separate observation. However, rule 1 and 3 are violated as there are no separate columns for total population and TB cases, and the cells in the "rate" column do not contain single values.*

`dplyr`, `ggplot2`, and all the other packages in the tidyverse are designed to work with tidy data.

## Wide vs long data

Unfortunately, most real data is untidy because researchers often organise data with its *collection* in mind, rather than its *analysis*. (Perhaps a good lesson for you to take forward with your own research is instead to think how your data could be organised with an analysis-centric mindset).

This means that most real analysis will require tidying, which in turn, requires **pivoting**. Pivoting is the process of converting between **wide** and **long** format.

**Wide** data is untidy but easier for humans to read. In wide data, each subject (or gene, airport, penguin...) is a row, while the measurements go into columns. An example we will come across in the next session is RNAseq count data, which may be structured similar to this:

![](images/clipboard-1903654721.png)

**Long** data is tidy and easier for R to read. It follows the 3 rules we outlined above, where each measurement gets its *own row*. We add extra columns to say *which* sample or condition it belongs to. E.g., if the RNAseq counts were above were **pivoted** into long format:

![](images/clipboard-2062496887.png)

`tidyr` provides two functions for pivoting data: `pivot_longer()` and `pivot_wider()`. We are only going to cover `pivot_longer()` here because it is far more common.

### `featurecounts` read statistics

To demonstrate the utility of `pivot_longer()` and bring together many of the things we have learned this session, we are going to look at some read statistics from an RNAseq experiment which has been conducted using the `featurecounts` function of the `Rsubread` package—much more on this next session! **Our goal is to make a plot comparing the number of assigned reads across all 8 samples.**

In your working directory, there should be a file called `fc.RDS`. This file was generated after running `featurecounts()` on aligned BAM files from an RNAseq experiment looking at the global transcriptional effects on brains from rats post traumatic brain injury (TBI). The data was uploaded to the NIH Bioproject repository under the accession name PRJNA902029, and the original paper is *Zhu, Xiaolu, Jin Cheng, Jiangtao Yu, Ruining Liu, Haoli Ma, and Yan Zhao. ‘Nicotinamide Mononucleotides Alleviated Neurological Impairment via Anti-Neuroinflammation in Traumatic Brain Injury’. International Journal of Medical Sciences 20, no. 3 (2023): 307–17. <https://doi.org/10.7150/ijms.80942>.*

We will dig in deeper to how this file was made and what it contains in our final session next week. For now, we will read it into R using the function we learned about earlier, `read_rds()`.

```{r}
fc <- read_rds("fc.rds")
```

We can have a look at this file using `View()` or `glimpse()`.

```{r}
#| eval: false
View(fc)
```

```{r}
glimpse(fc)
```

You will notice that this object contains a few subdivisions, named `counts`, `annotation`, `targets` and `stat`. We will come on to what these are in the next session.

For now, the information we require is contained within the `stat` component. One highly useful operator in R is the `$` sign, as this allows you to select different components within objects. Note: you can also use `$` to select specific columns in data frames—which I have not covered here as we are following a tidyverse-centric approach to working with data frames—but this is still a very useful feature in the right context.

We can use this to select the `stat` component of `fc` and assign it to a new object, which we will just call `stat`:

```{r}
stat <- fc$stat
stat
```

You can see from this that `stat` is another data frame. Is this **tidy?**

.

.

.

*Scroll down for answer*

.

.

.

.

.

.

.

.

.

.

This data frame contains separate samples in columns, and is therefore **wide** data. It would be very difficult to filter the data for just assigned reads and summarise the number of assigned reads across all samples.

In addition, all of those column names are unnecessarily long, so before we go any further, I am going to rename them using the `set_names()` function from `purrr` (also part of the tidyverse).

```{r}
stat <- stat %>%
  set_names(c("Status", "TBI2", "TBI1", "sham4", "sham3", 
              "sham2", "sham1", "TBI4", "TBI3"))

stat
```

To tidy this data so that we can apply the tidyverse functions and plot in `ggplot2`, we are going to use `pivot_longer()` to turn the data from wide format to long format.

```{r}
stat %>% 
  pivot_longer(
  cols = !Status,
  names_to = "Sample",
  values_to = "Count"
)
```

After the data, there are three key arguments:

-   `cols` specifies which columns need to be pivoted, i.e. which columns aren’t variables. This argument uses the same syntax as `select()`. Here we have said `!Status` as this contains our variables. The other columns are our samples, and need to be pivoted.

-   `names_to` names the variable stored in the column names; we named that variable `Sample`.

-   `values_to` names the variable stored in the cell values; we named that variable `Count`.

Note that our data is definitely *longer —* we have gone from a 14x9 table to a 112x3 table!

Note also that the new columns inherit the type of the previous columns or cells.

Pivoting our table in this way means it is much easier to now apply the tidyverse functions we have learned in this session. We are ready now to achieve our aim and **make a plot comparing the number of assigned reads across all 8 samples. Using all you have learned this session and the previous, have a go at writing the code to achieve this.**

Hint: You will need to

-   Pivot `stat` to make it longer with the appropriate arguments

-   Filter for only the `Assigned` read counts

-   Plot these counts using `ggplot2` with appropriate aesthetics and geoms. (If you want a colourblind palette, remember the `ggthemes` package from session 1).

.

.

.

*Scroll down for answer if you get stuck!*

.

.

.

.

.

.

.

.

.

.

.

.

```{r}
library(ggthemes) # loading ggthemes so we can use its colourblind-safe colour palette

stat %>% 
  pivot_longer(            # <- This section of the code converts stat to long format
  cols = !Status,
  names_to = "Sample",
  values_to = "Count"
) %>% 
  filter(Status == "Assigned") %>% # This function filters for Assigned reads
  ggplot(aes(x = Status, y = Count, fill = Sample)) + # ggplot code for plotting
  geom_col(position = "dodge") + 
  theme_minimal()+
  theme(
    axis.text.x = element_text(angle = 45, vjust = 0.6, face = "bold")
  ) +
  scale_fill_colorblind()

# I have chosen geom_col to show each sample, but there may be other appropriate geoms too!
```

# Summary

By the end of this session, you should now be able to:

-   Import data into R from different file types (`read_csv()`, `readRDS()`).

-   Understand the concept of *tidy data* and why it matters for analysis.

-   Reshape data from **wide** to **long** using `pivot_longer()`.

-   Use `left_join()` to combine data tables.

-   Apply `mutate()`, `select()`, `filter()`, `arrange()`, and `summarise()` to explore and transform data.

-   Handle NAs and inappropriate variable types.

-   Use the pipe (`%>%`) to chain steps together in a clear, readable workflow.

In our final bumper session, we will apply all that we have learned in sessions 1 and 2 to an R-based bioinformatics workflow using real world data from a published bulk RNAseq experiment.
