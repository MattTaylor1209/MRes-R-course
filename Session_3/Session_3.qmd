---
title: "Session 3 — Putting it all together: analysis of RNAseq data in R"
author: "Matthew Taylor"
execute:
  dpi: 300
  cache: false
  working-directory: project
format: 
  html:
    toc: true
    self-contained: true
editor: visual
---

Before we start, click on the little gear above the document window and select "clear all output" from the drop-down menu (this prevents any *spoilers!*):

![](images/clipboard-2475862271.png)

# 1 - Welcome

Welcome to the 3rd and final session in our *Foundations of R for Bioinformatics* course. In this session, we will finally turn our full attention to the latter part of that title and start applying what we have learned to a standard bulk RNA-sequencing workflow. This session is slightly longer that our previous two (3 hours instead of 2) but I don't want to completely fill our time with more and more information! So the actual content of this session should be roughly the same as the previous two, leaving us time at the end for practise and to go over any questions or parts of the course you found more challenging.

This session will *not* be covering the biology of RNA sequencing in much detail (hopefully you will already have had a lecture or workshop on that) except where relevant to the computational steps—rather, we will be focusing on the computational side of how you actually perform each step in an R-based environment.

## 1.1 - Aim

Our goal for today will be to walk through all of the different steps which are *typically performed* to analyse bulk RNAseq data (although specific workflows can vary) **from receiving the raw sequencing data** from the sequencing department/company, to basic **quality control**, through to **testing for differential gene expression and pathway enrichment.**

## 1.2 - Sources and inspirations

We are finally moving away from the Hadley Wickham [*R for Data Science(2nd Edition)*](https://r4ds.hadley.nz/) as their tutorial does not cover RNAseq. In 2-3 hours, we will only really be able to scratch the surface of what is possible when analysing RNAseq data in R, and there many online tutorials covering the enormous amount of tools available. In this session, we will be mainly using the [**Rsubread**](https://bioconductor.org/packages/3.21/bioc/vignettes/Rsubread/inst/doc/Rsubread.pdf "User guide for Rsubread") and [**DESeq2**](https://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html "User guide for DESeq2") packages; you can find the user guides (which we will be referring to) by clicking on those links. We will also take inspiration from the following 2 online courses:

-   ***RNAseq Analysis in R: Alignment and counting*** — Stephane Ballereau, Mark Dunning, Oscar Rueda, Ashley Sawle*.* <https://bioinformatics-core-shared-training.github.io/RNAseq-R/align-and-count.nb.html>

-   ***Analysis of RNAseq data in R*** — Sheffield Bioinformatics Core. <https://sbc.shef.ac.uk/rnaseq-r-online/>

As we are severely time limited, I will only be able to teach you the essentials which will help get you going through a standard bulk RNAseq workflow. For a highly-detailed, self-paced walkthrough of RNAseq analysis in R, I can also recommend the various courses from Harvard Bioinformatics Core—the one most relevant to our workshop today can be found [here](https://hbctraining.github.io/Intro-to-DGE/schedule/links-to-lessons.html), and it goes way beyond the scope of this course. You can also find links to other bioinformatics workflows there, such as for single-cell RNAseq.

## 1.3 - Goals for session 3:

By the end of this session, you will be able to:

-   Use **Rsubread** to **align** raw sequencing data to a reference genome, and **count** the number of reads per gene.

-   Perform basic quality control, including **filtering** and **clustering**.

-   Use **DESeq2** to perform **differential gene expression.**

-   Use the **tidyverse** to visualise the different outputs from our RNAseq data analysis, including **MA plots**, **volcano plots**, **heatmaps** etc.

-   *Perform basic pathway **over-representation analysis** using **clusterProfiler** (extension section)*

## 1.4 - Packages required (Make sure you run the code in this section at the beginning)

As before, we will be using the `tidyverse` and its associated packages. We also need some other packages which will aid us in the analysis and visualisation of our RNAseq data. We can install them now and load them as we go—I will explain what they are when we come to use them.

### 1.4.1 - Installing packages if not already installed:

Run the following chunk. Click "No" for any popup which appears. (You can safely ignore the error message if you get one).

```{r}
#| eval: false


packages_required <- c("tidyverse", "ggrepel", "ggthemes", "plotly", "pheatmap")

install.packages(packages_required)
```

### 1.4.2 - Loading the `tidyverse`, `ggrepel` and `ggthemes` packages

```{r}
library(tidyverse)
library(ggrepel)
library(ggthemes)
```

Up to this point, the data wrangling we have been doing can be broadly applied to many different fields of study—the first 2 sessions of this course would look very similar if I was teaching a group of Psychologists, for example. Thus we have been using packages which are hosted on [**CRAN**](https://cran.r-project.org/) (The Comprehensive R Archive Network). CRAN is where you first downloaded R itself, and it’s also the source that `install.packages()` searches when you supply a package name.

For today, however, we’ll also need packages that aren’t on CRAN. These come from a different, more specialised source. We will load these as we go.

### 1.4.3 - Bioconductor and BiocManager

Today, we are narrowing our focus to more specific, bioinformatics applications. The packages we will use today (e.g., `Rsubread`, `DESeq2`...) are therefore much more suited to biological data science, such as RNAseq. These packages are collected on the website [Bioconductor.org](https://www.bioconductor.org/), which has major releases twice a year, and their installation is slightly different.

**The chunk below installs all of the different Bioconductor packages we need for this session. You will get a popup asking "Do you want to install from sources the packages which need compilation?" Click "Yes". This code will take 10-15 minutes to complete and you will see lots of things happening in the console - don't worry about all this!**

*After a while, you will get the message in the console "Update all/some/none? \[a/s/n\]:". Type "n" and hit Enter. You're done!*

```{r install_bioconductor_packages}
#| eval: false
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

bioc_packages <- c("tidySummarizedExperiment", "DESeq2", "edgeR", "limma", "Rsubread"
                   )

BiocManager::install(bioc_packages)
```

The top part of the chunk checks whether `BiocManager` is installed (required for the installation of packages from Bioconductor.org) and if it isn't, it installs it. The middle part creates a new object containing a list of the names of the bioconductor packages we need for this session. The final part uses the `install()` function inside of `BiocManager` to install all of these packages.

Once installed, these packages are loaded in the same way as any other: using the `library()` function. We will do this later on when we need them.

# 2 - Alignment and counting

Let's say you've designed a really cool *in vitro* experiment which will look at the transcriptional responses of cells on drug X *vs* cells given a placebo. You've grown up your cells, split them into biological replicates, applied the drug for a certain amount of time, harvested the cells, extracted and purified the RNA, and sent the samples off for sequencing. The sequencing people have now got back to you with a link to download your data — you see these files with strange endings you might not have seen before (like *fastq.gz* — what does that mean?), and you also notice how large these files are; maybe tens of gigabytes? As you weep for your slow WIFI download speeds, you wonder to yourself: *what am I actually meant to do with all of this data?* This question will be the focus of this first section of today's session.

Firstly, the data — what on earth is a *fastq.gz* file? **FASTQ** is an evolution of the **FASTA** format for storing nucleotide sequences in a text-based format. The Q on the end refers to the addition of a "Quality" score, which is a way of representing the probability that the **base call** is correct or not. All sequencing is **probabilistic**, and encoding the quality score within the data allows the user to check a) how good the sequencing was overall and b) decide what cutoff to use when aligning the sequences. (The "gz" part of the name just means that the file is *compressed* using gzip, to save storage space).

Below shows an example workflow for generating bulk RNAseq data (adapted from Harvard Bioinformatics Core).

![](images/clipboard-2926418547.png){width="398"}

The sequencing occurs outside of an R environment at the sequencing facility using bespoke software depending on the manufacturer of the sequencer machine. This step involves taking your RNA samples and sequencing them to produce the *fastq* files containing all of the reads.

Much of the quality control also occurs outside of R. For example, it is quite common to use software called [**fastQC**](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) to get a picture of the sequencing quality and identify any samples that might need to be excluded e.g., if the RNA quality for those samples was poor due to degradation. Another common processing tool is [**fastp**](https://academic.oup.com/bioinformatics/article/34/17/i884/5093234) which performs multiple QC steps including adapter trimming, quality filtering, per-read quality pruning, poly-G trimming, and others.

Due to lack of time and the fact that these tools are not R-based (although, inevitably, there are packages which allow you to run [these](https://cran.r-project.org/web/packages/fastqcr/index.html "fastqcr link") [functions](https://www.bioconductor.org/packages/release/bioc/html/Rfastp.html "rfastp package link") in R), I will not be covering them in this session; however, I do **strongly recommend** that you take the time to follow the links and read about those tools in your own time, as quality control is a *crucial* step in RNA sequencing analysis.

## 2.1 - Alignment (read mapping)

The next step (orange box) is **alignment.** While this is also commonly performed outside of R, there are some excellent packages available which allow us to do this within R and they incorporate seamlessly into our downstream analyses. The package I will introduce to you today is called **Rsubread**, and as the title of its seminal paper says, it is "[easier, faster, cheaper and better for alignment and quantification of RNA sequencing reads](https://academic.oup.com/nar/article/47/8/e47/5345150)". What's not to like? (Of course, the authors of that paper *would* say that as they were the ones who developed *`Rsubread`*, but anyway...).

We will load this package now:

```{r rsubread}
library(Rsubread)
```

You can find a detailed user guide for *`Rsubread`* by running the command `RsubreadUsersGuide()`

```{r usersguide}
#| eval: false
RsubreadUsersGuide()
```

The basic principle of alignment is to take our reads and compare each sequence to a reference genome to see which genomic region each read matches the best (from [*Analysis of RNA-seq data in R*](https://sbc.shef.ac.uk/rnaseq-r-online/session1.nb.html#Learning_objectives_-_Session_1)):

![](images/clipboard-3863738.png)

The ingredients we need for successful **alignment** are:

-   Our *fastq* files

-   A reference index (i.e., a reference genome)

-   An aligner algorithm

### 2.1.1 - The catch

Unfortunately, alignment of reads to a genome is a *computationally-intensive process*. It's possible that the code will run on your grandma's dusty old Dell from 2009, but you might be waiting 7-10 business days for the result.

Some of you may be fortunate enough to have a powerful machine at home to do the alignment steps. However, I imagine many of you will be like me and do not have personal access to high performance computing. Fortunately, there are ways around this: here at Birmingham, research projects can be assigned access to the [BlueBEAR High Performance Clusters](https://www.birmingham.ac.uk/research/arc/bear/bluebear). Through their web portal, you can open RStudio in an interactive environment and run your scripts on there instead of your local PC, enabling you to take advantage of the processing power offered from BlueBEAR.

We won't be doing this today—instead, we will use the tutorial data directly contained within `Rsubread` as it is tiny and the code runs quickly on any device. But I thought I would point this out to you anyway.

TLDR: **alignment (and counting, although less so) are computationally intensive; I do not recommend you run those functions locally unless you are using a high-performance computer (e.g., a CPU with at least 8 cores) or have plenty of time on your hands. All other steps in this session are far less intensive and can be run on any low-mid range laptop from the last few years.**

::: callout-note
## Today, you will only run a tiny alignment workflow using Rsubread’s built-in tutorial FASTA and FASTQ files. You do *not* need to download any genomes or real FASTQ files.
:::

### 2.1.2 - the `buildindex()` function

The first step of alignment is to build an index from a reference genome. To do this, we need to use the `buildindex()` function, which is part of the `Rsubread` package. `buildindex()` has 2 required options: *`basename`,* which is the name you want to call the created index files, and *`reference`*, which is the name of a FASTA file containing the reference genome. Therefore, for your own experiments, you would need to download and provide a reference genome of whichever organism you are working on and extract it to your current working directory. These can be found on the [NCBI Datasets](https://www.ncbi.nlm.nih.gov/datasets/) website from the NIH National Library of Medicine.

**Example**: later we are going to be looking at some data from an RNAseq experiment using rats (*Rattus norvegicus*) which required alignment using the latest published rat genome at the time, **GRCr8**. To find this genome, I searched for "Rattus norvegicus" on the NCBI Datasets webpage: (**you [do not need]{.underline} to do these steps yourself right now**)

![](images/clipboard-1176517231.png)

This brought me to this page:

![](images/clipboard-2987091926.png)

Scrolling down, we have the option to download the reference genome:

![](images/clipboard-502302684.png)

You then get the following popup. Note that only "Genome sequences (FASTA)" is selected by default. I also selected "Annotation features (GTF)" as we will need that later.

![](images/clipboard-4066372499.png){alt="I then extracted the resulting files to the working directory. Building the index for alignment was then as simple as running the following code (no need to do this now):"}

I then extracted the downloaded files to my working directory.

Building the index for alignment was then as simple as running the following code (**don't run this chunk now as it will give an error—it is just here as an example for how you can build an index from genomic DNA of any particular species)**

```{r}
#| eval: false
buildindex(basename="GRCr8",reference="GCF_036323735.1_GRCr8_genomic.fa")
```

However, this process is also slow on low-end machines (especially for large mammalian genomes).

### 2.1.3 - Worked example for `buildindex()` - you can run the code in this section

Therefore, to demonstrate, we will use the built-in tutorial dataset from the *`Rsubread`* package:

```{r ref}
ref <- system.file("extdata","reference.fa",package="Rsubread")
# What have we just loaded?
ref
```

We have made a reference object we have called `ref` which points to a file within the *`Rsubread`* package called `reference.fa`. The path in the above chunk will look unique to each of you, but should all end in the same thing: `Rsubread/extdata/reference.fa`. Notice the file extension `.fa`. This tells us we have a FASTA file, which is what the *reference* option of `buildindex()` required.

Now we are ready to use `buildindex()`:

```{r buildindex}
buildindex(basename="reference_index",reference=ref)
```

All of the functions within `Rsubread` contain lengthy and useful output code, which prints to the console (and to your Quarto document) allowing you to see for yourself what is actually happening. Notice on my dodgy and slow Macbook Air, I get the message "WARNING: available memory is lower than 3.0 GB. The program may run very slow." This is why I am only using this very small tutorial reference to demonstrate the `buildindex()` function, as we would be here all day otherwise!

In our working directory, we now have some new files, all of which contain the prefix "reference_index" which came from our *`basename`* option above. You can see these files by clicking the "Files" pane on the right hand side of RStudio, or using the `list.files()` function.

```{r}
list.files()
```

You should see files ending in `.00.b.tab`, `.00.reads`, `.00.mapped`, etc. All of those files collectively make up our **reference index** which we will use next in the alignment step.

### 2.1.4 - the `align()` function

Alignment in `Rsubread` is generally performed using the `align()` function, which uses a paradigm called the "seed-and-vote" mapping paradigm. More information on this can be found in their published [paper](https://academic.oup.com/nar/article/47/8/e47/5345150) and the Users Guide.

Required options for the `align()` function are *`index`*, which points the aligner to the index we just built (specifically the *`basename`* we gave it) and *`readfile1` —* this can be a single *fastq* file **or** a list of such files. Note also there are many other options we can give to `align()` (see `help(align)` for details) including *`readfile2`* which is NULL by default. This is because sequencing data can be **single-end** or **paired-end**, and `align()` will assume single-ended sequencing data by default.

If we had paired-end data, we would make a folder containing our forward-read *fastq* files to use for *`readfile1`*, and another containing our reverse-read *fastq* files to use for *`readfile2`*. We can then call these files into the `align()` function as in the example below **(do not actually run this chunk; it is just here as an example)**:

```{r example_align_paired_end}
#| eval: false

# Make list of all of the names of our forward fastq files
fastq.fwd <- list.files("./fwd_files", full.names = TRUE) # object containing names of forward fastq files
# Make list of all of the names of our reverse fastq files
fastq.rev <- list.files("./rev_files", full.names = TRUE) # object containing names of reverse fastq files

# Run the alignment using our forward and reverse fastq files
align(index="GRCr8", readfile1=fastq.fwd, readfile2 = fastq.rev) # alignment using paired end sequencing data
```

If you are getting unusual results e.g., poor alignment, or errors, it is likely because there is some setting in the `align()` function which is inappropriate for your data. E.g., with paired-end data you need to make sure that your *fastq* files are properly paired (by having them in the same order in your forward and reverse folders).

It is therefore important to understand the format and structure of your data and ensure that you are using the appropriate settings in `align()`.

### 2.1.5 - Worked example for `align()` - you can run the code in this section

For our tutorial example, we will use a small file contained within the `Rsubread` package which pretends to be some reads from a dummy sequencing experiment:

```{r align-tutorial}
reads <- system.file("extdata","reads.txt.gz",package="Rsubread")

align.stat <- align(index="reference_index",readfile1=reads,
                    output_file="alignResults.BAM",phredOffset=64)
```

*Note that we have also specified an output file in this code, and a* `phredOffset` *which we don't need to worry about here.* Even on my pedestrian laptop, this code only takes a few seconds to run.

The output of the `align()` function will be a host of new files (as many as the number of unique *fastq* files, i.e., experimental samples) which are in the BAM file format (Binary Alignment Map). These are the files we need for the next step of the process — **counting.**

### 2.1.6 - Questions

1.  Have a look at the help page for the `align()` function—write `help(align)` in the console and press Enter. If you have more than one topic appear, it is the one titled "Align Sequence Reads to a Reference Genome via Seed-and-Vote".

    a\) If we were aligning on a more powerful PC, is there an option we could use to make it run faster? *Hint: Is there a way of increasing the number of CPU threads which the function uses?*

    b\) What if we were aligning genomic DNA rather than RNA? Is there an option for doing this?

2.  Look at the output of the *Worked example for `align()`* chunk.

    a\) How many reads were there in total?

    b\) How many were successfully mapped onto our reference index?

    c\) What percentage does this represent?

    d\) What are some reasons a read *might not* align?

3.  Use the `list.files()` function by writing it into a new chunk and running the chunk, or type `list.files()` into the console and press Enter. What new files do we have in our working directory? Do you see any files containing `BAM`? Where have they come from? Where did we specify their name?

## 2.2 - Counting

So far, we have taken our sequencing data and aligned (or attempted to align) each sequence to a region in the genome. We know that in our tutorial data, 904 out of 1000 reads were successfully assigned to genomic regions. However, at this stage, we do not know *where* the reads were aligned, and crucially for our quantitative RNA sequencing, *how many times* each genomic region was aligned to—i.e., the count of how many times each gene was *seen* in our data.

We therefore need some way of **counting** these features, a process known as ***read quantification**,* which produces a record of the number of occurrences of each feature in a ***count table***. Enter stage left: the `featureCounts()` function.

### 2.2.1 - `featureCounts()`

The `featureCounts()` function requires 2 inputs. The first is the *`files`* option which can be a single file or a list of files in the BAM format (which we will have generated from our alignment step). The second is an *`annotation`*, which is a list of **genomic features**—basically a way of saying what the reads actually correspond to on the gene level.

`featureCounts()` has some annotation files built-in, which can be accessed using the *`annot.inbuilt`* option (e.g., for mouse data, you could use `annot.inbuilt = "mm39"`). Alternatively, you can provide your own annotation file using the *`annot.ext`* option and pointing it to a GTF, GFF or SAF file.

E.g., from our rat TBI example above: I selected a GTF file to be included in my download, and we could use this in the `featureCounts()` step like this **(do not run the chunk below—it is for demonstration purposes only)**:

```{r rat_featurecounts}
#| eval: false
bam.files <- list.files(path = "./BAM_files", pattern = ".BAM$", full.names = TRUE) # make an object containing the names of our BAM files

# Run featureCounts using our list of BAM files and our external genomic annotation, and assign it to a new object called fc
fc <- featureCounts(files = bam.files, annot.ext = "GRCr8_genomic.gtf", isGTFAnnotationFile = TRUE)
```

Note this is another area where things can go wrong if the wrong options are used:

-   We have to specify `isGTFAnnotationFile = TRUE` if we are using a `.gtf` file as our annotation

-   If our data is paired-end, we also have to specify `isPairedEnd = TRUE` (default is FALSE)

There are a whole list of other options to `featureCounts()`— more detail of which (and their default settings) can be found via `help(featureCounts)`.

### 2.2.2 - Worked example for `featureCounts()` - you can run the code in this section

For our tiny tutorial data, we can create a small annotation tibble with some dummy genes, just to show how this can work in principle. The structure of this dummy annotation tibble is broadly similar to the structure of a GTF file:

```{r dummy_ann}
ann <- tibble(
   GeneID=c("gene1","gene1","gene2","gene2"),
   Chr="chr_dummy",
   Start=c(100,1000,3000,5000),
   End=c(500,1800,4000,5500),
   Strand=c("+","+","-","-")
)
ann
```

We will then perform the feature counting using this dummy annotation data frame. Since we are using our own annotation rather than one which is in-built to `featureCounts()`, we need to use the `annot.ext` option.

```{r fc_dummy}
fc_dummy <- featureCounts("alignResults.BAM",annot.ext=ann)
```

If we have a look at our new `fc_dummy` object (by clicking on it in the Environment tab, using `glimpse(fc_dummy)` or using `View(fc_dummy)`, we can see that it it divided into 4 sections: `counts`, `annotation`, `targets` and `stat`.

```{r}
glimpse(fc_dummy)
```

We will dig into what these mean in the following sections.

Note that on large datasets (not our dummy tutorial dataset), `featureCounts()` can take a long time on an average personal computer, so this is another function which benefits from being run on a high performance computer cluster, such as [**BlueBEAR**](https://www.birmingham.ac.uk/research/arc/bear/bluebear). Once run, the `featureCounts` object which is generated can be saved as an RDS file using the tidyverse function `write_rds()` and loaded elsewhere for downstream analysis (`read_rds()`). This requires much less computing power than generating the `featureCounts` object every time you want to do any analysis. For example, with our tutorial `fc_dummy`:

```{r save_rds}
#| eval: false

# To save the RDS file for use later
write_rds(fc_dummy, "fc_dummy.RDS") 

# To load it back into an R object

fc_dummy <- read_rds("fc_dummy.RDS")

```

From here on, we will be working with a `featureCounts` RDS file generated elsewhere using the steps described in this section.

### 2.2.3 - Questions

1.  Have a look at the help page for the `featureCounts()` function—write `help(featureCounts)` in the console and press Enter. If you have more than one topic appear, it is the one titled "Count Reads by Genomic Features".

    a\) As before with `align()`, is there a way we could speed the process up on a more powerful computer?

    b\) Sometimes, reads will map to multiple locations in the genome, and these are counted by default. What should we do if we do *not* wish to count these reads?

    c\) What are the different inbuilt annotations available? How many different species do they cover?

2.  Look at the output of the *Worked example for `featureCounts()`* chunk.

    a\) What was the total number of alignments which `featureCounts()` attempted to count? Where have you seen this before?

    b\) How many were successfully assigned (counted)?

    c\) What is the percentage?

    d\) Why is this figure so much lower than the number of aligned reads from before?

3.  Why do we need an annotation file? Why would we not just count using our genomic reference index we built earlier?

## 2.3 - The `featureCounts` object

For the remainder of this session, we are going to be working with an RDS file which I briefly introduced to you at the end of last week's session.

If you downloaded the entire course as one zip file, then in your working directory for Session_3 (where this document is located) there should be a file called `fc.RDS`. This was generated after **alignment** (using the `align()` function from `Rsubread`) and **counting** (using `featureCounts()` on those aligned BAM files).

The raw sequencing data originated from an RNAseq experiment investigating the **global transcriptional changes in rat brain samples post traumatic brain injury (TBI)**. For our purposes, I selected only the "sham" group and the "TBI" (injured) group—**each group contains 4 samples**.

For reference, the data were uploaded to the NIH Bioproject repository under the accession name PRJNA902029, and the original paper is *Zhu, Xiaolu, Jin Cheng, Jiangtao Yu, Ruining Liu, Haoli Ma, and Yan Zhao. ‘Nicotinamide Mononucleotides Alleviated Neurological Impairment via Anti-Neuroinflammation in Traumatic Brain Injury’. International Journal of Medical Sciences 20, no. 3 (2023): 307–17. <https://doi.org/10.7150/ijms.80942>.*

**Run the code below to load this data into R:**

```{r fc}
fc <- read_rds("fc.rds")
```

We can take a peek at what's inside our `fc` object by using `glimpse(fc)`. The `glimpse()` function is handy here, because `fc` is a really large object. If you just typed `fc` into a chunk and ran it, you might find it crashes RStudio (or takes ages to load!). We will use `glimpse()` whenever we need to peek at large R objects.

```{r glimpse_fc}
glimpse(fc)
```

If you find the output a bit messy and hard to interpret, you can also open `fc` by clicking on it in the right hand side 'Environment' panel. You will then see something like this:

![](images/clipboard-101443889.png)

See, for example, that `counts` and `annotation` have 38675 rows—it would therefore be difficult to view these subdivisions without `glimpse()` and is a **good example of why we use R for this process** (and not, say, Excel).

We can also use the `names()` function we encountered in Session 1 to see what the different subdivisions of `fc` are:

```{r names}
names(fc)
```

We can see from this that `fc` contains 4 sub-divisions: `counts`, `annotation`,`targets`, and `stat`. Recall that in the last session, I introduced you to the `$` operator, which allows you to access these subdivisions within larger objects, like `fc`. So if we wanted to select just the `counts` subdivision of `fc`, we could write `fc$counts`.

### 2.3.1 - `counts`: count data

`counts` contains our count data. Every row is a unique genomic feature (mostly genes), and every column is one of our samples.

What are the names of our samples? It turns out every column of `counts` is labelled with the sample it refers to. We can look at column names using the `colnames()` function:

```{r colnames-fc-counts}
colnames(fc$counts)
```

These names reflect the 2 treatment groups in our rat TBI experiment (sham and TBI), with 4 samples in each group.

### 2.3.2 - `targets`: samples

There is another way we could have seen the names of our samples: they are contained in the `targets` subdivision of `fc`.

```{r fc-targets}
fc$targets
```

### 2.3.3 - `annotation`: genomic annotation data

`annotation` is a data frame containing the genomic annotation data:

```{r fc-annotation}
glimpse(fc$annotation)
```

Unsurprisingly, it has 38675 rows (one for each genomic feature), and 6 columns, which contain the GeneID, Chromosome, Start, End, Strand, and Gene Length.

### 2.3.4 - `stat`: read statistics and basic quality control

Finally, `stat` we explored last session, which contains the read statistics for each of our samples:

```{r fc-stat}
fc$stat
```

This subdivision is useful for quality control, as it can flag outliers (e.g., if you have a sample where the RNA quality was poor).

At the end of last week's session, we explored how we can use `stat` to perform basic quality control by plotting the numbers of assigned reads per sample using `ggplot()`. Let's recap that now. First we will make a new **tibble** object called `stat` by assigning `fc$stat` to this name:

```{r stat}
stat <- tibble(fc$stat)

#have a look at our new tibble
stat
```

I mentioned last week the need for data to be **tidy** in order for `tidyverse` functions, such as `ggplot()`, to work. This new tibble is **untidy**, as it contains samples in separate columns.

Ideally, we would have a column called `sample` which would contain all of our samples within it. We can then create another column called `count` which will contain all of the counts for each sample for each `Status` e.g., count of reads `Assigned`, `Unassigned_Unmapped` etc...

To do this, we use a **pivot** function called `pivot_longer()`. *If you are not sure what the code below does, don't worry*—we are only using it once just for this quick quality control step, and if you ever have your own data from `Rsubread` you can literally just paste this code in and it should work, providing you have made a `stat` tibble like we have done above:

```{r stat_tidy}
stat_tidy <- stat %>%
    pivot_longer(
    cols = !Status,        # all columns except Status
    names_to = "Sample",   # name of the new “sample” column
    values_to = "Count"    # name of the new values column
  )

# let's look at what we've done
stat_tidy
```

Now every row contains just one observation and all of our samples are in one column. Our data is now **tidy** and ready for use in `ggplot()`. The code below is based on the answer to the final challenge from last week, taking our new `stat_tidy` data, filtering for just the `Assigned` reads and plotting this per sample as a bar chart:

```{r assigned-reads-plot}
library(ggthemes) # loading ggthemes so we can use its colourblind-safe colour palette

stat_tidy %>% 
  filter(Status == "Assigned") %>% # This function filters for Assigned reads
  ggplot(aes(x = Status, y = Count, fill = Sample)) + # ggplot code for plotting
  geom_col(position = "dodge",alpha = 0.8) +
  scale_fill_colorblind() # our colourblind friendly palette
```

### 2.3.5 - Questions

1.  `fc$counts` contains all of the counts of all of the genes from all of the samples (i.e., the count data). Run the following code:

    ```{r}
    dim(fc$counts)
    ```

    a\) How many rows are there? What does each row correspond to?

    b\) How many columns are there? What does each column correspond to?

2.  Use your data transformation skills from last week to figure out which sample contained the highest number of `Unassigned_Unmapped` reads. *Hint: first filter for just Unassigned_Unmapped reads then arrange the count from highest to lowest*:

    ```{r}
    stat_tidy %>%
      filter(YOUR_CODE_HERE) %>%
      arrange(YOUR_CODE_HERE)
    ```

# 3 - Differential gene expression with `DESeq2`

There are numerous fantastic tools for downstream analysis of RNAseq data, including [*edgeR*](https://bioconductor.org/packages/release/bioc/html/edgeR.html), [*limma*](https://bioconductor.org/packages/release/bioc/html/limma.html)*,* and [*DESeq2*](https://bioconductor.org/packages/release/bioc/html/DESeq2.html)*,* to name a few. All of these tools have similar design ideas—they take raw read counts, apply some kind of normalisation to the data, and perform something called [empirical Bayes shrinkage](https://en.wikipedia.org/wiki/Empirical_Bayes_method) to stabilise variance estimates across genes. The subtle differences between them come later—`edgeR` and `DESeq2` assume that the data follow a [negative binomial distribution](https://en.wikipedia.org/wiki/Negative_binomial_distribution), while `limma` involves transforming the data to log counts per million and applying linear modelling.

We will be using **`DESeq2`** as it is particularly well suited to dealing with small sample sizes, which is what you will likely be faced with if you ever perform your own RNAseq experiments. A highly detailed walkthrough of how to use `DESeq2` to analyse RNAseq data can be found in its [*vignette*](https://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) on Bioconductor. *Note that if you do look at that for reference, our tutorial starts from the section where it talks about using count matrices as input. Alternatively, a slower-paced walkthrough, which also includes a different way to generate count data from* fastq *files, can be found [here](https://master.bioconductor.org/packages/release/workflows/vignettes/rnaseqGene/inst/doc/rnaseqGene.html).*

Of course, to use `DESeq2`, we first need to load the package as usual using the `library()` function. We are also going to borrow a function from `edgeR` a bit later on, so we will load this here too:

```{r edgeR-DESeq2}
library(edgeR)
library(DESeq2)
```

Loading these two packages gives us quite a lot of text in the output, because they depend on a variety of other packages which are installed and loaded at the same time. You can also see where certain functions are "masked" from other packages—this basically means that two or more packages contain functions of the same name, and R will only use one of them by default.

**Because we are using a wider variety of packages in this session—some of which contained identically-named functions—you will now see me using the syntax `package::function()` to specify exactly which package I am using the function from! This is actually good practice to do *all the time.***

### A note on "tidyness"

This whole course, we have had a *tidyverse-centric* mindset to our data analysis. However, some packages in R (most of the Bioconductor packages included) are not designed to be naturally compatible with the tidyverse. The *`DESeqDataSet`,* which we will come to in a moment, is a subset of the class of R objects called *`SummarizedExperiment`* objects: by default, it is *not*, for example, a **tibble**, which as we learned last week is a **tidy** representation of a data frame.

Fortunately, "where there is a will, there is a package." The *`tidySummarizedExperiment`* package causes a *`DESeqDataSet`* to be treated a tibble for tidyverse function purposes, while still allowing normal *`DESeq2`* functions to work properly.

```{r tidySummarizedExperiment}
library(tidySummarizedExperiment)
```

## The Goal

Our goal for the **remainder of the session** (sections 3-6) is to perform differential gene expression analysis on some real-world data and produce the following visual representation of differentially-expressed genes—a **volcano plot**. This is one of the most common data visualisation techniques following RNAseq as allows for a clear visual summary of the differences in gene expression between 2 groups with the most significant genes labelled for future follow up. By the end of today's session, you will be able to produce a plot which looks like this:

![](images/clipboard-216378500.png)

## 3.1 - Count data input

*`DESeq2`* requires un-normalised count data as input, as library size differences are corrected internally. **Where is that stored in our `fc`** **object? Fix the code below.**

```{r student-answer-3.1}
#| error: TRUE

fc$YOUR_CODE_HERE
```

.

.

.

.

*Scroll down for answer*

.

.

.

.

.

.

.

.

.

.

.

.

We have already seen that `fc$counts` contains a matrix where the rows are the genes and the columns are the samples, with each value being a count of how many times that gene was detected in that sample. This is our count data, and because it is big, I would advise viewing it using the `glimpse()` function.

```{r model-answer-3.1}
glimpse(fc$counts)
```

We can therefore use this as the counts input for *DESeq2*—we will make a new object called `countdata` which will contain this count matrix:

```{r countdata}
countdata <- fc$counts
glimpse(countdata)
```

Using `glimpse()` shows us that our new `countdata` object is the same format as `fc$counts`.

### 3.1.1 - Assigning group information (factors) to our `countdata`

If we look at the column names of `countdata`, we can see that it corresponds to the column names we saw earlier in `fc$counts`:

```{r colnames-countdata}
colnames(countdata)
```

Although we can clearly see which group each sample belongs to, R currently can't—we need to make it explicit.

It is generally recommended to include information (**metadata**) about your experiment in a separate object, which you can refer to yourself and also use for providing information to *`DESeq2`* about the experimental design. Because we only have 8 samples, it is very straightforward to just make this object directly in R, as a tibble. (There's also no reason you couldn't make this as a CSV file in Excel if you prefer, especially if you have more samples and more groups).

We will use the `tibble()` function to make a 8x2 table, with a `SampleName` column containing the column names of `countdata` and a `Group` column assigning each of those samples to a specific group. We also want this variable to be a **factor** as we are treating it as a categorical variable:

```{r sampleinfo}
sampleinfo <- tibble(
  SampleName = c("TBI2" , "TBI1" , "sham4" ,"sham3", "sham2", "sham1", "TBI4" , "TBI3"),
  Group = factor(c("TBI", "TBI", "sham", "sham", "sham", "sham", "TBI", "TBI"))
)

sampleinfo
```

We've designed it like this so that the `SampleName` column in `sampleinfo` is in the same order as the columns in `countdata`, as we can check below using the `table()` function's equality test:

```{r table-colnames}
table(colnames(countdata)==sampleinfo$SampleName)

# If this is at all FALSE, you will need to remake sampleinfo to ensure that it is listing the samples in the same order as in countdata
```

### 3.1.2 - The DESeqDataSet object

In *`DESeq2`*, read counts and the intermediate values generated during analysis are stored in a special object called a **DESeqDataSet**, usually represented in code as an object `dds`. A *`DESeqDataSet`* can be generated in different ways; but here, we will generate it using our count matrix, `countdata`.

Every `DESeqDataSet` requires a **design formula**, which specifies the variables to be included in the model. We only have one variable of interest, which is a column in our `sampleinfo` tibble: `Group`. This variable is how we will distinguish the samples which came from the "sham" controls from those which came from the "TBI" injured animals. When we made the `sampleinfo` tibble, we made this column a `factor` variable type, as we want to use it as a **categorical variable.**

The design formula begins with a tilde (`~`), followed by the all the variables you want to consider separated by `+` signs (if you have more than one variable). E.g., if we had the variables `Group`, `Diet`, `Sex`, our design could be:

`design = ~ Group + Diet + Sex`

You can change the design later, but keep in mind that all differential expression steps must then be re-run, since the design formula is used when estimating dispersions and calculating log2 fold changes.

Because we are creating a *`DESeqDataSet`* from a count matrix, we need to use the (very robustly named) function `DESeqDataSetFromMatrix()`. As inputs to this function, we need to provide the **counts matrix**, the **information** **about the samples** (the columns of the count matrix) **as a data frame/tibble**, and the **design formula**.

With the count matrix, `countdata`, and the sample information, `sampleinfo`, we can construct a *`DESeqDataSet`*. Our design uses the `Group` column from our `sampleinfo` object (i.e., that is the independent variable which differential gene expression is measured against—the differences between the groups).

*Don't worry if you get a warning from the chunk below. It relates to a function which hasn't been updated in `tidySummarizedExperiment` but it doesn't seem to cause any issues.*

```{r dds-deseqdatasetfrommatrix}
dds <- DESeqDataSetFromMatrix(countData = countdata,
                              colData = sampleinfo,
                              design = ~ Group)
dds
```

Because we loaded *`tidySummarizedExperiment`*, our *`DESeqDataSet`* is exposed as a *SummarizedExperiment–tibble abstraction*. That means we can use tidyverse functions for exploration while the object still holds assays (`counts`), row metadata (`genes`), and column metadata (`samples`). The preview reports **38,675 features** and **8 samples**—the same dimensions as our count matrix. Row names are **Gene IDs**; column names are our **sample IDs**.

### 3.1.3 - Exercises: Exploring `dds` using `dplyr` functions

We can use some of those tidyverse functions we learned about last week to explore our `dds` object a little bit to understand it a bit better.

**Look at the following functions and predict what will happen for each one (you can note it down on your version of this document). Run the chunk. Did the results match your prediction?**

*Note: I am using the prefix **dplyr::** because I want to specify the use of tidyverse functions and not base R (or other package) functions of the same name.*

#### `filter()`

```{r filter}
dds %>% 
  dplyr::filter(Group == "TBI")
```

#### `select()`

```{r select}
dds %>%
  dplyr::select(.sample)
```

#### `count()`

```{r count}
dds %>%
  dplyr::count(.sample)
```

#### `distinct()`

```{r distinct}
dds %>%
  dplyr::distinct(.sample, SampleName, Group)
```

#### `group_by()` and `summarise()`

```{r group_by-summarise}
dds %>%
    dplyr::group_by(.sample) %>%
    dplyr::summarise(total_counts=sum(counts))
```

#### `group_by()`, `mutate()` and `filter()`

```{r group_by-mutate-filter}
dds %>%
    dplyr::group_by(.feature) %>%
    dplyr::mutate(mean_count=mean(counts)) %>%
    dplyr::filter(mean_count > 10)
```

### 3.1.4 - Plotting

Thanks to `tidySummarizedExperiment`, we can also treat our `dds` object as a normal tibble for plotting in `ggplot2`. For example, if we wanted to plot the distribution of counts per sample (on a log10 scale), **which is a very common QC step**:

```{r count-distribution}
library(ggthemes)
dds %>%
    ggplot(aes(counts + 1, group=.sample, color=Group)) +# +1 to counts to avoid zeros
    geom_density() + # using a density plot geom
    scale_x_log10() + 
    theme_minimal() +
    scale_colour_colorblind()
```

### 3.1.5 - Pre-filtering

You can see from the plot we made above that our samples have a peak on the left hand side of the plot—this represents genes with very low counts.

While it is not necessary to pre-filter low count genes before running the *`DESeq2`* functions, there are two reasons which make pre-filtering useful: by removing rows in which there are very few reads, we reduce the memory size of the `dds` data object, and we increase the speed of count modeling within *`DESeq2`*. It can also improve visualizations, as features with no information for differential expression are not plotted in dispersion plots or MA-plots.

Pre-filtering can be performed manually. For example, one common method is to keep only rows that have a count of at least 10 for a minimal number of samples. A recommendation for the minimal number of samples is to specify the smallest group size, e.g., for our example, that would be n = 4. The *`DESeq2`* vignette provides [an example of how to do this](https://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#pre-filtering).

For automatic pre-filtering, we can borrow a function from the *`edgeR`* package, called `filterByExpr()`. We will call every row where we want to keep the data "keep" and use this to filter our `dds` object:

```{r filterByExpr}
# Check dimensions before filtering
dim(dds)

# Make an object called keep which will determine whether a gene should be kept (TRUE) or not (FALSE)
keep <- filterByExpr(counts(dds), group = dds$Group)

```

What does `keep` look like?

```{r keep}
head(keep) # just print the first few positions of keep
```

You can see we now have what is known as a *named vector*—every element of the vector `keep` corresponds to a `GeneID`, and the value is a *logical* (i.e., `TRUE` or `FALSE`). We can use this to filter our `dds` object to keep only the rows where this value is true. (Here it makes way more sense to *not* use a tidyverse approach but to rather use the matrix subsetting tools we learned about all the way back near the beginning of *Session 1*):

```{r dds-keep}
dds <- dds[keep,] # select only the ROWS where the gene ID is TRUE in our keep vector
```

### 3.1.6 - Factor levels

The `Group` variable in our *`DESeqDataSet`* tells *`DESeq2`* which samples belong to which experimental group. Because R orders factor levels alphabetically by default, it’s good practice to explicitly set the reference level so that *`DESeq2`* compares other groups against it. In *Session 1,* we briefly encountered the need to change the levels of a factor when we wanted to switch the order that our different penguin species were plotted on the graph. We can do a similar thing here to set our reference level, the `relevel` (our "sham" group):

```{r fct_relevel}
dds <- dds %>%
  dplyr::mutate(Group = fct_relevel(Group, "sham")) #fct_relevel means factor reference level
```

The easiest way to see if this has worked is to use the function `unique()` and access the `Group` variable using `dds$Group`:

```{r unique}
unique(dds$Group)
```

### 3.1.7 - Questions

1.  After filtering, how many genes have we kept? *Hint: the `dim()`* *function will give you the number of rows and columns of any R object.*

    ```{r}

    ```

2.  How does filtering affect our density plot which we made in **3.1.4**? Try plotting it again for yourselves:

    ```{r}

    ```

# 4 - Data quality assessment - demonstration

Data quality assessment and quality control (i.e. the removal of insufficiently good data) are essential steps of any data analysis. These steps should typically be performed very early in the analysis of a new data set, preceding or in parallel to the differential expression testing.

We define the term *quality* as *fitness for purpose*. Our purpose is the detection of differentially expressed genes, and we are looking in particular for samples whose experimental treatment suffered from an anormality that renders the data points obtained from these particular samples detrimental to our purpose.

## 4.1 - Checking the data - PCA

A **principle components analysis (PCA)** is an example of an unsupervised analysis, where we don’t specify the grouping of the samples. If your experiment is well controlled and has worked well, we should see that replicate samples cluster closely, whilst the greatest sources of variation in the data should be between treatments/sample groups. It is also an incredibly useful tool for checking for outliers and batch effects.

### Worked example

To run the PCA we should first normalise our data for library size and transform to a log scale. *`DESeq2`* provides two commands that can be used to do this: here we will use the command *`rlog()`*. *`rlog()`* performs a log2 scale transformation in a way that compensates for differences between samples for genes with low read count and also normalizes between samples for library size.

```{r rlog}
rld <- rlog(dds, blind = TRUE)
```

Next, we will use this as our input for running the PCA, using the `plotPCA()` function from `DESeq2`. To save computational power, this function will by default only use the top 500 most variable features to calculate the PCA. In addition, we can specify `pcsToUse` to select the principle components we want to analyse (defaults to 1 and 2).

```{r plotPCA}
plotPCA(rld, intgroup = "Group", # intgroup specifies a variable we are splitting the samples by: in our case, Group 
        pcsToUse = 1:2 # pcsToUse specifies which principle components to plot (default is first 2)
        ) 
```

This plot is actually a `ggplot` object, meaning we can customise it to look however we want. First, we extract the PCA data itself by using the option `returnData = TRUE` inside `plotPCA()`.

```{r pca_data}
pca_data <- plotPCA(rld, intgroup = "Group", pcsToUse = 1:2, returnData = TRUE) 
```

We also need a small vector which contains the percentage variance explained by each principle component, which will be useful for labelling our x and y axes.

```{r percentVar}
percentVar <- round(100 * attr(pca_data, "percentVar"), 2) # round to 2 dp
```

Now we are ready to plot using `ggplot2`. The following chunk contains a lot of code but try to read it line by line to figure out what each part is doing. I have added in the `geom_text_repel` geom from the `ggrepel` package, which enables labelling of the points with the sample names. I am also specifying which shape I want to be in the legend using the `guides()` layer. **Have a go at changing some of the values in the geoms—what effects do your changes have on the figure?**

```{r pca_plot-ggplot}
library(ggrepel)
pca_plot <- pca_data %>%
  ggplot(aes(x = PC1, y = PC2, fill = Group, shape = Group)) + # plot PC1 and PC2
  geom_point(size = 5) +
  geom_text_repel(aes(label = SampleName), size = 4, # Add sample labels
                  box.padding = 1,      # Increases space around labels
                  point.padding = 1,    # Increases distance from points
                  min.segment.length = 0) + # Minimum distance from label before it draws a line connector
  scale_shape_manual(values = c(21, 24)) + # Change this depending on how many shapes you want (i.e., how many groups), shapes21-25 are decent
  guides(fill = guide_legend(override.aes = list(shape = 22))) + # specify the shape I want the shape in the legend to be
  theme_minimal() + # remove some of the plot background
  labs( # set the labels i.e., title, x and y axes labels. 
    title = "PCA Plot of RNA-seq Samples",
    x = paste0("PC1 (", percentVar[1], "% variance)"), # paste0 pastes text together
    y = paste0("PC2 (", percentVar[2], "% variance)")
  )

pca_plot
```

Quick note on `paste0`—this is a great way to *dynamically label* our axes depending on values calculated elsewhere. In our example, we have calculated the percentage variance explained by each PC and stored it in a vector we called `percentVar`. We could just look at that vector and find the relevant numbers and manually label our axes as such. However, using `paste0`, we don't need to, as it will automatically grab whatever we put inside of it (so if we changed the parameters and calculated the percentage variance again, we wouldn't need to manually change the axes labels).

Just to make it explicit what `paste0` is doing:

```{r paste0}
paste0("PC1 (", percentVar[1], "% variance)")
```

If we wanted to manually say how much variance was explained:

```{r paste0-2}
paste0("PC1 (", 81.53, "% variance)")
```

Let's look at our PCA plot again:

```{r pca-plot-full}
pca_plot
```

This plot is telling us that about 82% of our variance is explained on the x axis (by PC1), and about 6% is explained on the y axis (PC2). This is expected, as the variance explained decreases with each principle component. The distance between samples on the plot corresponds to how similar or different they are from each other. The good news is that our samples seem to broadly cluster together by their `Group`, with sham samples on the right and TBI samples on the left.

## 4.2 - Checking the data - heatmaps

A **heatmap** is a colour-coded representation of a data matrix—in RNA-seq, typically **genes (rows)** by **samples (columns)**, with colours indicating expression levels (often after transformation such as *rlog* or *vst*).

Before diving into differential expression, heatmaps help you visually check data structure and sample relationships. They are useful for:

1.  **Detecting sample clustering and group separation**

    -   When you cluster both rows (genes) and columns (samples), samples from the same experimental group should cluster together.

    -   If a sample clusters with the wrong group, that can suggest **sample mislabelling**, **batch effects**, or **outliers**.

2.  **Identifying outlier samples**

    -   An outlier sample will show a **distinct colour pattern** (e.g., consistently higher or lower expression across most genes).

    -   This can flag **technical issues** such as low library complexity, contamination, or failed normalisation.

3.  **Visualising high-variance or most variable genes**

    -   Heatmaps of the top 500–1000 most variable genes give an overview of biological variability versus noise.

    -   It’s a quick way to see whether biological signal dominates technical variation.

### Worked example

We will use the `pheatmap` library for plotting our heatmap. Unfortunately, it isn't completely tidyverse friendly, but we can still use our tidyverse functions and the `%>%` pipe to prepare our data for `pheatmap`. We will use the `rld` object which contains an rlog transformation of our `dds` *`DESeqDataSet`.*

Firstly, we will extract the rlog-normalised counts from `rld` using `assay()`:

```{r normalized-counts}
# Extract the transformed values
normalized_counts <- assay(rld)
glimpse(normalized_counts)
```

We want to plot the 500 most variable genes in our dataset. To do this, we will calculate the row variances using `rowVars`, ensure it is a **tibble** with the gene IDs in their own column, and arrange by variance from highest to lowest:

```{r variance-per-gene}
# Compute variance per gene
row_variances <- rowVars(normalized_counts) %>% 
  as_tibble(rownames = "GeneID") %>% 
  dplyr::arrange(desc(value))

row_variances
```

Next, we extract the GeneIDs for the top 500 most variable genes in our dataset. Since we have arranged our `row_variances` tibble from most to least variable, we can use a `slice` function—specifically `slice_head()` to extract from the top:

```{r geneIDs}
# Identify top 500 most variable genes
topVarGenes <- row_variances %>%
  dplyr::slice_head(n = 500) %>% # 500 because we want the top 500 most variable genes
  pull(GeneID) # we "pull" because we only need the names of these genes, which is contained in the GeneID column

head(topVarGenes) # look at the top most variable
```

Finally, we can use this to subset our `normalized_counts` matrix as input for `pheatmap()`. Note that `pheatmap()` requires a matrix as input so we cannot convert `normalized_counts` to a tidyverse friendly tibble. This means we will have to use a matrix subsetting technique, like we did earlier this session*.* We use the `%in%` function because we are looking for the row names of `normalized_counts` which match the names of our top variable `GeneIDs`.

```{r heatmap}
library(pheatmap)

# Subset and plot
pheatmap(
  normalized_counts[rownames(normalized_counts) %in% topVarGenes, ],
  cluster_rows = TRUE,
  cluster_cols = TRUE,
  show_rownames = FALSE # with this many genes, would be too messy if set to TRUE
)
```

In this heatmap, each row is a gene and each column is a sample. We have designated that we want to cluster by rows and columns (feel free to experiment with what happens if you set either or both of those to FALSE)—so genes with most similar expression are closest together, and samples with most similar patterns of gene expression are closest together. Blue cells indicate lower gene expression, while red cells indicate higher gene expression.

We can see, like our PCA, that our samples have clustered together by their `Group` (i.e., all of our shams are next to each other and all of our TBIs are next to each other). We also can see that generally the expression of genes is well conserved across each sample in each group (e.g., genes that are lowly expressed in one sham sample are also lowly expressed in the other sham samples) meaning that the variation is due to `Group` differences and not `sample` differences.

# 5 - Differential expression analysis

After the basic quality control steps we have performed above, we are now ready to start the differential expression analysis. The standard steps for this are contained within the function `DESeq()`. For detailed information on how this function performs differential expression analysis, see the help pages using `help(DESeq)` and the original paper [here](https://pmc.ncbi.nlm.nih.gov/articles/PMC4302049/). You will find various different options in the help page, but I personally normally just use the default settings!

## 5.1 - `DESeq()`

**Performing differential expression analysis is as simple as running the code in the following chunk.** Note that we want to overwrite `dds` with our differential expression analysis, so we use the assignment operator `<-`:

```{r deseq}
dds <- DESeq(dds)
```

## 5.2 - `results()`

The `results()` function extracts a result table from a *`DESeq`* analysis giving **base means** across samples, **log2 fold change**, **standard errors**, **test statistics**, **p-values**, and **adjusted p-values** for each gene.

If you run `results()` without any extra options, `DESeq2` will automatically use the **last variable** in your design formula to define the comparison. If that variable is a factor, `DESeq2` compares the **last level** of the factor against its **reference level** (see earlier note on setting factor levels).

For example, our design was (see [3.1.2 - The DESeqDataSet object]):

`design = ~ Group`

Our `Group` variable had the levels "sham" and "TBI" (with "sham" as the reference). Running `results(dds)` will test **TBI vs sham** and return the log2 fold change for that comparison.

If you have more than one variable in your design, or want to test a specific contrast, you can specify it explicitly using either the `contrast` option. For instance

```{r results}
res <- results(dds, contrast = c("Group", "TBI", "sham"))
res
```

When you use the **`contrast`** option, `DESeq2` will automatically set the **log2 fold change (LFC)** to **0** if *both groups being compared have all counts equal to zero*, even if other groups in the dataset have nonzero counts.

This behaviour can be useful, since it avoids reporting undefined or misleading fold changes in cases where no reads were detected in either group. If you want this automatic handling of zero-count comparisons, use the `contrast` option to create your results table.

As always, more detail on the `results()` function and the options available can be found using `help(results)`

## 5.3 - `summary()`

To get some summary statistics on our results, we can use the `summary()` function:

```{r summary}
summary(res)
```

**How many adjusted p-values were less than 0.1 (i.e., significant as defined by the default settings)? Fix the code below.**

```{r student-answer-5.3}
#| error: true
res %>%
  as_tibble() %>%
  dplyr::filter(YOUR_CODE_HERE) %>%
  dplyr::summarise(total = "total", n = n())
```

.

.

.

.

Scroll down for answer

.

.

.

.

.

.

.

.

.

.

.

```{r model-answer-5.3}
res %>%
  as_tibble() %>%
  dplyr::filter(padj < 0.1) %>%
  dplyr::summarise(total = "total", n = n())
```

*Might need to click the "show in new window" button but you should hopefully see a count of **7419**.*

## 5.4 - log2 fold change thresholds and false discovery rate

By default, `results()` does not take into account the **magnitude** of the log2 fold change—i.e., it will test for significant differential expression regardless of whether the log2 fold change is tiny (and potentially biologically insignificant). In addition, the default adjusted p-value cutoff (i.e., the false discovery rate (FDR), ***alpha***) is 0.1. If you want to be more **stringent**, you specify a higher log2 fold change threshold, `lfcThreshold` (e.g., 1) and a lower `alpha` value (e.g., 0.05). We shall do this below and save it to an object called `res05`:

```{r res05}
res05 <- results(dds, lfcThreshold=1, alpha = 0.05, contrast=c("Group","TBI","sham"))
res05
```

**How do you predict this will affect the number of significant differentially-expressed genes we discover? How can you tell if you are right? Fix the code below to check.**

```{r student-answer-5.4}
#| error: true

summary(YOUR_CODE_HERE)
```

.

.

.

*Scroll down for answer*

.

.

.

.

.

.

.

.

.

.

.

```{r model-answer-5.4}
summary(res05)
```

*As you can see, there are much fewer significant genes than when we used the default settings—we have been more **stringent.***

### 5.4.1 - MA plots

The MA plot is a visualisation that plots the log2-fold-change between experimental groups (M) against the mean expression across all the samples (A) for each gene. *`DESeq2`* contains a function called `plotMA()` for this purpose. We can use this to visualise the effects of different parameters in `results()`.

Points are coloured in blue if their adjusted p-value is below the *`alpha`* value specified in `results()`. If we have specified a log2 fold change cutoff, we can add horizontal lines to the plot depicting that using the function `abline()`.

```{r plotMA}
DESeq2::plotMA(res, ylim = c(-5, 5))
```

We can see the effect of restricting the log2 fold change threshold and false discovery rate if we make an MA plot from our `res05` object:

```{r draw-line}
DESeq2::plotMA(res05, ylim = c(-5, 5))
abline(h=c(-1,1),col="dodgerblue",lwd=2) # function to draw lines onto a plot
```

Notice how fewer points are now coloured in blue (as we were more stringent with our adjusted p-value threshold) and none of them are contained within the horizontal blue lines (the log2 fold change threshold).

## 5.5 - Directionality of hypothesis testing

The `results()` function also allows you to specify which direction you want to test for differences in gene expression—e.g., you might (for some reason) only care about genes which *increase* in expression above a certain log2 fold change. The `altHypothesis` option in `results()` allows you to specify the specific values of log2 fold change you are interested in finding. If the log2 fold change specified by *name* or by *contrast* is taken as ***β**,* then the possible values for `altHypothesis` represent the following alternate hypotheses:

-   "greaterAbs": **\|*β*\| \> lfcThreshold**. p-values are two-tailed. The default and most common.

-   "lessAbs": **\|*β*\| \< lfcThreshold**. p-values are maximum of the upper and lower tests (I don't personally know why you would ever use this one)

-   "greater": ***β*** **\> lfcThreshold**

-   "less": ***β*** **\< -lfcThreshold**

The four possible values of `altHypothesis` are demonstrated in the following code and visually by MA-plots in the following chunk:

```{r althypothesis}
ylim <- c(-5, 5)
resGA <- results(dds, lfcThreshold=1, altHypothesis="greaterAbs")
resLA <- results(dds, lfcThreshold=1, altHypothesis="lessAbs")
resG <- results(dds, lfcThreshold=1, altHypothesis="greater")
resL <- results(dds, lfcThreshold=1, altHypothesis="less")
drawLines <- function() abline(h=c(-1,1),col="dodgerblue",lwd=2)
DESeq2::plotMA(resGA, ylim=ylim); drawLines()
DESeq2::plotMA(resLA, ylim=ylim); drawLines()
DESeq2::plotMA(resG, ylim=ylim); drawLines()
DESeq2::plotMA(resL, ylim=ylim); drawLines()
```

## 5.6 - Putting it all together

Putting this together, **say you wanted to test for DEGs along the following parameters of \|log2FC\| \> 0.5 and FDR \< 0.05 comparing the TBI group vs the sham group:** **how would you fix the code below?**

```{r student-answer-5.6}
#| error: true

res <- results(dds, lfcThreshold=YOUR_CODE_HERE, alpha = YOUR_CODE_HERE, contrast=c("Group","TBI","sham"))

res
```

.

.

.

.

*Scroll down for answer*

.

.

.

.

.

.

.

.

.

.

.

.

.

```{r model-answer-5.6}
res <- results(dds, lfcThreshold=0.5, alpha = 0.05, contrast=c("Group","TBI","sham"))
res
```

And produce an MA plot to visualise:

```{r plotMA-res}
DESeq2::plotMA(res, ylim = c(-8, 8))
abline(h=c(-0.5,0.5),col="dodgerblue",lwd=2) 
```

## 5.7 - `summary()` revisited

To get a summary of our results object, `res`:

```{r summary-res}
summary(res)
```

We've seen this summary a few times now but it's worth digging in to what it's telling us:

The first line says we have a total of 17138 genes which we are testing for differential expression. We pre-filtered our data to ensure we had no genes with a read count of zero, but it's worth knowing that *`DESeq2`* would have excluded them anyway, as seen here.

The next line tells us what we had set as our FDR: i.e., an adjusted p-value cutoff of 0.05.

The following 2 lines are self explanatory: we have 914 significant genes with a positive log2 fold change of at least 0.5, which is 5.3% of our total genes; and we have 451 significant genes with a negative log2 fold change of at least -0.5, which is 2.6% of our total genes.

26 genes have been flagged as outliers, as identified by a statistical test called [Cook's distance](https://en.wikipedia.org/wiki/Cook%27s_distance). Here, it's application is to flag genes where the observed counts do not fit a Negative Binomial distribution, which is one of the core assumptions of *`DESeq2`*'s statistical model.

We have 0 genes with low counts (defined here as mean count \< 7); again, this is likely due to our pre-filtering step.

In the next and final section, we will be working primarily with this `res` object and seeing how we can extract useful information from it and visualise the data using common plotting techniques.

# 6 - Exploring and exporting results from differential gene expression

## 6.1 - Exploring the `res` object

If you click on the `res` object in the right hand Environment sidebar, you can get some information about what this object is by default. For example, you can see that its object class is *`DESeqResults`* which is a special class of object unique to the *`DESeq2`* package. It also tells us that we have 17138 rows, which are the number of genes that have been tested for differential expression in total (not the number of *significant* genes).

Of particular use is the **metadata** subsection, which contains our **alpha** and **lfcThreshold—**handy in case you had lost or forgotten the parameters you had set when making the object. It's worth keeping this version of the object around, but unfortunately a *`DESeqResults`* object does not play nicely with tidyverse functions.

### 6.1.1 - Making `res` tidyverse-friendly

To make our data exploration and export more convenient and tidyverse-friendly, we will make a **tibble** version of our `res` object. Recall from last week that a tibble is basically an improved version of the basic R data frame. This object unfortunately doen not contain all of the extra information and metadata which `res` contains, which is why we are not overwriting `res` directly. We shall call this new object `res_tbl`. We will also just replace any NA values in the `padj` column with 1.

```{r res_tbl}
res_tbl <- res %>% 
  as_tibble(rownames = "GeneID") %>%
  dplyr::mutate(padj = replace_na(padj, 1))
```

We use the option `rownames = "GeneID"` because in `res`, our rownames corresponded to the GeneID and we don't want to lose this information, so instead we will store this information as a new column.

```{r look-at-res_tbl}
res_tbl
```

You can see from the preview that this object has the columns `GeneID`, `baseMean`, `log2FoldChange`, `lfcSE` (standard error), `stat`, `pvalue` and `padj`. At the moment, the table is just ordered in its default arrangement, which just happens to be the order of the `GeneIDs` from our GTF file from earlier.

A more useful ordering might be by adjusted p-value, from smallest to largest (so that our most significant genes are at the top:

```{r arrange}
res_tbl <- res_tbl %>% 
  dplyr::arrange(padj)

res_tbl
```

### 6.1.2 - Exporting results as a CSV file

If we just want to save data table of all of our results from the differential gene expression, we could export this table as a CSV using the `write_csv` function:

```{r write_csv}
write_csv(res_tbl, "full_results.csv")
```

This is now saved in our current working directory, and can be opened in Excel or shared with your less R-confident colleagues and friends!

### 6.1.3 - Questions

1.  You might just want to save only the genes which show significant differential gene expression, i.e., those with a FDR (adjusted p-value) of less than your accepted alpha value. Say we want to filter for genes with adjusted p-value of less than 0.05. **How could we do this using a tidyverse function we learned about last session? Fix the code below.**

```{r}
#| error: true

res_sig <- res_tbl %>%
  dplyr::filter(YOUR_CODE_HERE)

res_sig

```

2.  How would you export this as a CSV file?
3.  Fix the code below to create a new object, `resOrdered`, which is made by sorting `res_tbl` by `log2FoldChange` from highest to lowest? (Bonus points if you can do it by the absolute value of `log2FoldChange`)

```{r}
#| error: true

resOrdered <- res_tbl %>%
  dplyr::arrange(YOUR_CODE_HERE)

resOrdered
```

## 6.2 - Volcano plots

### 6.2.1 - Using `ggplot2`

A **volcano plot** is a type of **scatter plot** which displays the relationship between **fold change** (on the x-axis) and **statistical significance**, typically represented by the **negative log of the p-value** (on the y-axis), for each gene in the dataset. Genes with large changes in expression (either upregulated or downregulated) and high statistical significance are found further from the origin, typically forming the “wings” of the volcano shape. The plot helps to quickly identify genes that are both highly differentially expressed and statistically significant, which are potential candidates for further study.

We can visualise our results as a volcano plot in a various ways, but the prettiest (in my opinion) and most flexible is using *`ggplot2`*. We will build this plot and add features step by step, a bit like how we did for our `penguins` dataset in *Session 1.*

Let's set up a basic plot. We will use our `res_tbl` object and use this to create a *`ggplot`* object we will call `volcano_plot`. Firstly, we will specify our axes. We want `log2FoldChange` on the x-axis, and `-log10(adjusted p-value)` on the y-axis. Make sure we use the correct column names from our `res_tbl` object.

```{r volcano_plot_data}
volcano_plot <- res_tbl %>%
  ggplot(aes(x = log2FoldChange, y = -log10(padj)))

volcano_plot
```

We now have our blank canvas. Next, we need our data points (i.e., our genes). We want to depict these as points. **What geom do we need for this?**

```{r student-answer-6.2.1}
#| error: true

volcano_plot + YOUR_CODE_HERE

```

.

.

.

.

*Scroll down for answer*

.

.

.

.

.

.

.

.

.

.

.

.

.

The geom we need is `geom_point()`. Note that because `volcano_plot` is a *`ggplot`* object, we can just write in `volcano_plot` and add layers to it with `+`:

```{r model-answer-6.2.1}
volcano_plot + geom_point()

```

You can already start to see a volcano shape taking place!

One common visualisation techique which people like to add to volcano plots is to colour the points based on significance and directionality. For example, we might decide to:

-   Colour significantly upregulated genes red (LFC \> 0.5 and padj \< 0.05)

-   Colour significantly downregulated genes blue (LFC \< -0.5 and padj \< 0.05)

-   Colour all other genes grey

The easiest way to do this is to add a new column to our original data (which we can call anything but let's call it "significance" as this makes the most sense). We will say that the values of this column can take one of three values—either "`upregulated`", "`downregulated`" or "`not significant`". We will then colour the points based on the value within this table.

I'm going to use something called a "[conditional statement](https://intro2r.com/conditional-statements.html)". We don't have the time to go into this in great detail, which is why I would recommend following the links in this paragraph for more information. But at a glance, conditional statements allow you to tell R to do one thing *if* something is `TRUE`, and do something *else* if that something is `FALSE`. The most common use of this is the [`ifelse`](https://r4ds.hadley.nz/logicals.html#if_else)`()` function. The [`case_when`](https://r4ds.hadley.nz/logicals.html#case_when)`()` function is an extension of this, allowing you to have more than the default of 2 different conditions.

Let's now use `case_when()` to add this new column to our `res_tbl` based on the criteria we set in our bullet points above:

```{r case-when}
res_tbl <- res_tbl %>%
  mutate(significance = case_when(
    padj < 0.05 & log2FoldChange > 0.5 ~ "Upregulated",
    padj < 0.05 & log2FoldChange < -0.5 ~ "Downregulated",
    TRUE                            ~ "Not significant"
  ))
```

We can now use this column to add colour to our points:

```{r volcano-coloured}
volcano_plot <- res_tbl %>%
  ggplot(aes(x = log2FoldChange, y = -log10(padj))) +
  geom_point(aes(color = significance))

volcano_plot
```

This is almost what we were after (we have colour) but not quite. R has just given us some default colours, which are not the ones we were after. To change the colours, we need to provide a **scale**. Scales are a really useful tool in *`ggplot`*: they control *how* data values are mapped to visual properties (aesthetics) of the plot. In other words:

-   You tell `ggplot` *which variable* controls colour, shape, size, etc.

-   The **scale** tells `ggplot` *how to translate the data values* into those visual properties.

As an analogy, imagine you have temperature readings and a set of paint colours. The *aesthetic mapping* tells `ggplot` which variable is “temperature.” The *scale* tells `ggplot` exactly which colours correspond to which temperatures.

We want to map colour to significance, and we have 3 different options (`upregulated`, `downregulated` and `not significant`)—this means we need 3 different colours. We will provide this using `scale_colour_manual()`:

```{r scale_colour_manual}
volcano_plot <- volcano_plot + 
  scale_color_manual(values = c("blue", "grey", "red")) # Colours: grey for not significant, red for up-regulated, blue for down-regulated

volcano_plot
```

Things are really starting to come together! Another common visualisation tactic on volcano plots is to put dashed vertical and horizontal lines at your p-value and log2 fold change cutoffs. This can be achieved using `geom_hline()` for horizontal lines, and `geom_vline()` for vertical lines:

```{r lines}
volcano_plot <- volcano_plot + 
  geom_hline(yintercept = -log10(0.05), linetype = "dashed", color = "black") +  # Horizontal line at p-value threshold
  geom_vline(xintercept = c(-0.5, 0.5), linetype = "dashed", color = "black")  # Vertical lines at LFC = -0.5 and 0.5

volcano_plot
```

This is a useful plot for visualising roughly how many significant differentially expressed genes we have in our experiment, and whether they are up- or down-regulated. However, from this, we can't actually tell what those genes are.

One final addition to our plot would be to label a sensible number of the most significant genes using text labels. Let's say we want to label the top 20 most significant genes. This time we can use the `ifelse()` function because we only have 2 options: "`label`" or "`no label`". We can also deploy another technique from *Session 2*: the `%in%` operator.

*The following code will only work if we've already ordered our `res_tbl`* *object from most to least significant by p-value, so make sure you have done that first. It's the 3rd code chunk of section 6.1.1.*

```{r labels}
volcano_plot <- volcano_plot + 
  geom_text_repel(aes(label = ifelse(GeneID %in% GeneID[1:20], GeneID, NA)), size = 3, max.overlaps = 10) # labelling top 20 genes

volcano_plot
  
```

***Quick note—don't run the above chunk more than once, otherwise you will get a duplication of your labels as ggplot tries to add more layers of text on the graph!***

Finally, we will tidy up the axes labels and remove the grey background from the plot using `theme_minimal()`. There are actually an enormous amount of adjustments you can make to the look of a plot using [`theme()`](https://ggplot2.tidyverse.org/reference/theme.html) which we don't have time to go into today, but worth checking out—all of the options available can be found at that link or by using `help(theme)`.

```{r adding_theme}
#| warning: false
volcano_plot <- volcano_plot +
  theme_minimal() +
  labs(title = "Volcano Plot", x = "Log2 Fold Change", y = "-Log10 Adjusted P-value", 
       color = "Gene Expression Change")

volcano_plot
```

### Final Challenge:

**Putting it together, what is the full code we would use to create this volcano plot from scratch rather than step-by-step?**

```{r student-answer-final-challenge}
#| error: true
volcano_plot <- res_tbl %>%
  ggplot(aes(YOUR_CODE_HERE)) +
  geom_point(aes(YOUR_CODE_HERE)) +
scale_color_manual(YOUR_CODE_HERE) + 
  geom_hline(YOUR_CODE_HERE) +  
  geom_vline(YOUR_CODE_HERE) +  
  geom_text_repel(YOUR_CODE_HERE) + # labelling top 20 genes
  theme_minimal() +
  labs(title = "Volcano Plot", x = "Log2 Fold Change", y = "-Log10 Adjusted P-value", 
       color = "Gene Expression Change") # setting our theme and labels
  
volcano_plot

```

.

.

.

.

*Scroll down for answer*

.

.

.

.

.

.

.

.

.

.

.

.

```{r model-answer-10}
#| warning: false
volcano_plot <- res_tbl %>%
  ggplot(aes(x = log2FoldChange, y = -log10(padj))) +
  geom_point(aes(color = significance)) +
scale_color_manual(values = c("blue", "grey", "red")) + 
  geom_hline(yintercept = -log10(0.05), linetype = "dashed", color = "black") +  
  geom_vline(xintercept = c(-0.5, 0.5), linetype = "dashed", color = "black") + 
  geom_text_repel(aes(label = ifelse(GeneID %in% GeneID[1:20], GeneID, NA)), size = 3, max.overlaps = 10) + # labelling top 20 genes
  theme_minimal() +
  labs(title = "Volcano Plot", x = "Log2 Fold Change", y = "-Log10 Adjusted P-value", 
       color = "Gene Expression Change") # setting our theme and labels
  
volcano_plot
```

We now have something that looks like a publication quality volcano plot, which gives a clear visual overview of the number of genes significantly up and downregulated in our TBI samples compared to the sham control, with the most significant genes labelled.

### Bonus: Interactive volcano plot using `plotly` (for demonstration)

There are other packages which allow you to go further with this and make volcano plots which are interactive. For example, wouldn't it be useful if you could mouse over any of those other points and find out what gene it corresponds to? It is possible to do this using the `plotly` package. This doesn't use the same syntax as `ggplot` so I won't go over exactly what is going on in the chunk below (although you may be able to figure it out yourselves)—but I wanted to at least give you a glimpse of what is possible:

```{r plotly-volcano}
library(plotly)

plotly_volcano <- plot_ly(
  data = res_tbl, 
  type = "scatter",
  x = ~log2FoldChange, 
  y = ~-log10(padj),
  text = ~paste("Gene: ", GeneID, # this section controls the text you see when you mouse over
                "<br>Log2FC: ", signif(log2FoldChange, 3), 
                "<br>P-adj: ", signif(padj, 5)),
  color = ~significance, # will colour by the significance variable we made earlier
  colors = c("blue","grey", "red"),
  mode = "markers",
  marker = list(size = 5)
) %>%
  layout(
    title = "Interactive Volcano Plot",
    xaxis = list(title = "Log2 Fold Change"),
    yaxis = list(title = "-Log10 Adjusted P-value"),
    shapes = list(
      list(type = "line", x0 = -0.5, x1 = -0.5, y0 = 0, y1 = max(-log10(res_tbl$padj)), line = list(dash = "dash")),
      list(type = "line", x0 = 0.5, x1 = 0.5, y0 = 0, y1 = max(-log10(res_tbl$padj)), line = list(dash = "dash")),
      list(type = "line", y0 = -log10(0.05), y1 = -log10(0.05), x0 = min(res_tbl$log2FoldChange), x1 = max(res_tbl$log2FoldChange), line = list(dash = "dash"))
    )
  )

# Show the plot
plotly_volcano
```

If you mouse over the points above, you should now see some information about the gene it corresponds to. That's pretty cool, right?!

This plot is still interactive even when you render this document into a HTML file and open it in a web browser, which makes sharing these results with your colleague/supervisor/friend/granny extremely simple.

# Wrap-up

With that, we have come to the end of our three part course on *Foundations of R for Bioinformatics*. We have covered an awful lot of ground in a short space of time (especially today) and I do not expect you all to now be experts in R programming (I certainly am not!). But hopefully you now feel a bit more confident about tackling your own RNAseq analysis if ever you need to—and maybe can apply your new found R skills to all sorts of other data analysis you will come across in your career.

These Quarto notebooks are yours to keep and do whatever you like with! Hopefully, the code is sufficiently detailed that you could adapt it to your own experiment with very few changes required. With how I have written these documents, I am hopeful they could provide a framework for your own RNAseq data analysis, should you ever do so. In most cases, you can just copy the code over to a new R script or Quarto document, maybe just change the names of the samples or files (e.g., if you are using a different genome to rat), and it should just work. Copying other people's code and tweaking it for your own purposes is the basis of computer programming anyway!

If you would like more in depth information on anything I've covered, please follow the links which I have put throughout these notebooks—there are so many more manuals, worked examples and tutorials out there that I have barely scratched the surface of. It is also easy to find [communities](https://forum.posit.co/) of like-minded [researchers](https://support.bioconductor.org/), [coders](https://stackoverflow.com/questions), data-analysts, [bioinformaticians](https://www.biostars.org/)—the list goes on—who are passionate about open source data science and helping people with their code and data analyses. And if you're really stuck...well I guess there's always ChatGPT!

**If there is time still left in this session, please let me know if there was any particular area you would like me to go over again, as I am scheduled to be here for the full 3 hours!**

# Session Info

```{r}
sessionInfo()
```
